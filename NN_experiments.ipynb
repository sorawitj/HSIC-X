{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e2d847-91be-4f37-8247-adbfff1e4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.extend(['DeepGMM'])\n",
    "\n",
    "from DeepGMM.methods.toy_model_selection_method import ToyModelSelectionMethod\n",
    "from kernel import CategoryKernel, RBFKernel\n",
    "from model import train_HSIC_IV, NonlinearModel, train_mse\n",
    "import pandas as pd\n",
    "from utils import med_sigma, to_torch, gen_data, fit_restart, gen_radial_fn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4805fb-bc82-4ca1-8fec-4e58829489cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "n_rep = 8\n",
    "n = 1000\n",
    "num_basis = 10\n",
    "data_limits = (-7, 7)\n",
    "\n",
    "config_hsic = {'batch_size': 256, 'lr': 5e-2, \n",
    "               'max_epoch': 700, 'num_restart': 1}\n",
    "\n",
    "config_mse = {'batch_size': 256, 'lr': 5e-2, \n",
    "              'max_epoch': 300}\n",
    "\n",
    "f = gen_radial_fn(num_basis=num_basis, data_limits=data_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a0b560-1049-42bf-ae04-29e0d692fd46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9491878725332269\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7973788055822338\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0008624847958145626 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0003339656069959308 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0003749015649348753 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.584403, dev-loss=-0.661850, mean-recent-eval=-0.221306\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.271197, dev-loss=0.277111, mean-recent-eval=-0.204337\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.086104, dev-loss=0.095005, mean-recent-eval=-0.093789\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.008279, dev-loss=0.010597, mean-recent-eval=-0.022449\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.001086, dev-loss=0.000971, mean-recent-eval=-0.001204\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.001952, dev-loss=0.001025, mean-recent-eval=-0.000583\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.002892, dev-loss=0.002638, mean-recent-eval=-0.000613\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.004012, dev-loss=0.004927, mean-recent-eval=-0.000730\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.005224, dev-loss=0.006673, mean-recent-eval=-0.000799\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.006018, dev-loss=0.007351, mean-recent-eval=-0.000883\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.006859, dev-loss=0.008970, mean-recent-eval=-0.000985\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.007440, dev-loss=0.009650, mean-recent-eval=-0.001074\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.007583, dev-loss=0.009767, mean-recent-eval=-0.001072\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.007061, dev-loss=0.008467, mean-recent-eval=-0.001133\n",
      "best iteration: 500\n",
      "p-value: 0.8401655416867199\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.6215870480946841\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0005963264164339752 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0008255852293154174 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -9.161355735208846e-05 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.284496, dev-loss=-0.171156, mean-recent-eval=-0.006275\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.004903, dev-loss=0.002869, mean-recent-eval=-0.016161\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.027227, dev-loss=0.013920, mean-recent-eval=-0.009704\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.002197, dev-loss=-0.002031, mean-recent-eval=-0.000280\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.000441, dev-loss=-0.000709, mean-recent-eval=-0.000561\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.000886, dev-loss=-0.000218, mean-recent-eval=-0.000727\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.001030, dev-loss=-0.000514, mean-recent-eval=-0.000720\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.001230, dev-loss=-0.000344, mean-recent-eval=-0.000725\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.001528, dev-loss=-0.000539, mean-recent-eval=-0.000776\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001787, dev-loss=-0.000761, mean-recent-eval=-0.000758\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.001979, dev-loss=0.001221, mean-recent-eval=-0.000792\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002456, dev-loss=-0.001015, mean-recent-eval=-0.000825\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002697, dev-loss=-0.000692, mean-recent-eval=-0.000820\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.003057, dev-loss=-0.000723, mean-recent-eval=-0.000851\n",
      "best iteration: 280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.8987982886495935\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8856752610415047\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.000152941788921957 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.000186436692880201 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.00022527235791368195 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.096508, dev-loss=0.972433, mean-recent-eval=-0.042228\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.000174, dev-loss=0.002517, mean-recent-eval=-0.005997\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.001226, dev-loss=0.003522, mean-recent-eval=-0.000402\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.001711, dev-loss=-0.001060, mean-recent-eval=-0.000381\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.001689, dev-loss=0.001843, mean-recent-eval=-0.000371\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.001829, dev-loss=-0.001879, mean-recent-eval=-0.000274\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.002345, dev-loss=-0.001351, mean-recent-eval=-0.000280\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.002018, dev-loss=-0.001524, mean-recent-eval=-0.000231\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.002011, dev-loss=-0.001045, mean-recent-eval=-0.000233\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.002318, dev-loss=0.001316, mean-recent-eval=-0.000269\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.002312, dev-loss=0.001342, mean-recent-eval=-0.000229\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002245, dev-loss=0.001919, mean-recent-eval=-0.000187\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002126, dev-loss=-0.000827, mean-recent-eval=-0.000190\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.002292, dev-loss=0.001327, mean-recent-eval=-0.000222\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.002671, dev-loss=-0.000538, mean-recent-eval=-0.000216\n",
      "best iteration: 1020\n",
      "p-value: 0.9789795266378774\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9536177278239936\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0004852664303004201 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0027681368396178757 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0029378950938100837 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.202925, dev-loss=-0.210468, mean-recent-eval=-0.385548\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.072988, dev-loss=0.094648, mean-recent-eval=-0.156788\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.000506, dev-loss=-0.002571, mean-recent-eval=-0.007711\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.001116, dev-loss=-0.004719, mean-recent-eval=-0.002557\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.001298, dev-loss=-0.001469, mean-recent-eval=-0.002534\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.001655, dev-loss=-0.000288, mean-recent-eval=-0.002632\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.002047, dev-loss=0.000239, mean-recent-eval=-0.002683\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.002191, dev-loss=-0.005759, mean-recent-eval=-0.002559\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.002745, dev-loss=0.001995, mean-recent-eval=-0.002589\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.002577, dev-loss=-0.005960, mean-recent-eval=-0.002537\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.002624, dev-loss=-0.005310, mean-recent-eval=-0.002338\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002676, dev-loss=-0.001713, mean-recent-eval=-0.002433\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002780, dev-loss=-0.005565, mean-recent-eval=-0.002243\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.002699, dev-loss=-0.001497, mean-recent-eval=-0.002352\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.002681, dev-loss=-0.003386, mean-recent-eval=-0.002399\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.002668, dev-loss=-0.003857, mean-recent-eval=-0.002056\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.002961, dev-loss=-0.006834, mean-recent-eval=-0.001939\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.002915, dev-loss=-0.005826, mean-recent-eval=-0.001903\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.002895, dev-loss=-0.006665, mean-recent-eval=-0.001743\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.003038, dev-loss=0.000115, mean-recent-eval=-0.001750\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.002697, dev-loss=-0.002008, mean-recent-eval=-0.001783\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.002573, dev-loss=-0.003507, mean-recent-eval=-0.001628\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.002692, dev-loss=-0.002263, mean-recent-eval=-0.001672\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.002829, dev-loss=-0.005217, mean-recent-eval=-0.001420\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.002721, dev-loss=-0.005500, mean-recent-eval=-0.001401\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.002871, dev-loss=-0.007423, mean-recent-eval=-0.001342\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.002486, dev-loss=-0.001972, mean-recent-eval=-0.001465\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.002599, dev-loss=-0.000565, mean-recent-eval=-0.001285\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.002503, dev-loss=-0.000556, mean-recent-eval=-0.001278\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.002409, dev-loss=-0.003409, mean-recent-eval=-0.001238\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.002561, dev-loss=0.000124, mean-recent-eval=-0.001359\n",
      "best iteration: 2640\n",
      "p-value: 0.7328791587333738\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7639405037085171\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0037718317581127217 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0036239248360101887 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.003721974353939195 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-1.664392, dev-loss=-1.874549, mean-recent-eval=-0.421864\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.692270, dev-loss=0.780429, mean-recent-eval=-0.443292\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.477262, dev-loss=0.560083, mean-recent-eval=-0.343412\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.254191, dev-loss=0.317177, mean-recent-eval=-0.231354\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.091306, dev-loss=0.128440, mean-recent-eval=-0.122360\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.016471, dev-loss=0.024834, mean-recent-eval=-0.037992\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.011377, dev-loss=0.005962, mean-recent-eval=-0.005923\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.010103, dev-loss=0.004900, mean-recent-eval=-0.004599\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.009347, dev-loss=0.008958, mean-recent-eval=-0.004817\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007969, dev-loss=0.003837, mean-recent-eval=-0.004612\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.007167, dev-loss=0.007076, mean-recent-eval=-0.004405\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.007189, dev-loss=0.007794, mean-recent-eval=-0.004235\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.005845, dev-loss=0.003622, mean-recent-eval=-0.003918\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.005241, dev-loss=0.000908, mean-recent-eval=-0.003832\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.004656, dev-loss=0.001450, mean-recent-eval=-0.003509\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.004174, dev-loss=-0.000939, mean-recent-eval=-0.003712\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.003496, dev-loss=0.000837, mean-recent-eval=-0.003544\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.003330, dev-loss=0.002769, mean-recent-eval=-0.003566\n",
      "best iteration: 1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.7871027341716591\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7277282544123775\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -4.9698088391228647e-05 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.00019463940916631333 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.00013508531632816447 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.388351, dev-loss=0.372539, mean-recent-eval=-0.154714\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.004729, dev-loss=0.002104, mean-recent-eval=-0.035392\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.000488, dev-loss=-0.000293, mean-recent-eval=-0.000112\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.000323, dev-loss=0.000349, mean-recent-eval=-0.000193\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.000204, dev-loss=0.000574, mean-recent-eval=-0.000217\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.000473, dev-loss=-0.000737, mean-recent-eval=-0.000206\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.000278, dev-loss=0.000684, mean-recent-eval=-0.000291\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.000252, dev-loss=-0.000191, mean-recent-eval=-0.000303\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.000439, dev-loss=0.002044, mean-recent-eval=-0.000425\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000511, dev-loss=0.000301, mean-recent-eval=-0.000385\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.001124, dev-loss=-0.001258, mean-recent-eval=-0.000384\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000462, dev-loss=-0.000066, mean-recent-eval=-0.000383\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000949, dev-loss=-0.001172, mean-recent-eval=-0.000360\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000645, dev-loss=-0.000721, mean-recent-eval=-0.000370\n",
      "best iteration: 120\n",
      "p-value: 0.8408118196808575\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7743646215294022\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.00012468921509509035 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0002357310472219174 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0002683110002753908 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.100978, dev-loss=1.088339, mean-recent-eval=-0.236282\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.319242, dev-loss=0.312711, mean-recent-eval=-0.201502\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.221011, dev-loss=0.221402, mean-recent-eval=-0.141647\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.131918, dev-loss=0.132521, mean-recent-eval=-0.084294\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.057616, dev-loss=0.058485, mean-recent-eval=-0.041302\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.013304, dev-loss=0.014346, mean-recent-eval=-0.013900\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.000545, dev-loss=0.001728, mean-recent-eval=-0.001444\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.000578, dev-loss=0.001427, mean-recent-eval=0.000263\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.000728, dev-loss=0.001691, mean-recent-eval=0.000266\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000630, dev-loss=0.001127, mean-recent-eval=0.000262\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000645, dev-loss=0.000939, mean-recent-eval=0.000263\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000771, dev-loss=0.001169, mean-recent-eval=0.000260\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000803, dev-loss=0.001167, mean-recent-eval=0.000257\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000774, dev-loss=0.000985, mean-recent-eval=0.000252\n",
      "best iteration: 780\n",
      "p-value: 0.9757195600465979\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7316481193297674\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.028759052641792383 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02837171585296731 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.020684960919945067 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.160797, dev-loss=0.182176, mean-recent-eval=-0.077170\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.039958, dev-loss=0.011021, mean-recent-eval=0.003119\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.039449, dev-loss=0.008602, mean-recent-eval=0.023183\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.040654, dev-loss=0.008564, mean-recent-eval=0.024544\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.039204, dev-loss=0.010078, mean-recent-eval=0.025920\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.033889, dev-loss=0.008281, mean-recent-eval=0.027318\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.027878, dev-loss=0.000601, mean-recent-eval=0.028735\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.025012, dev-loss=-0.004805, mean-recent-eval=0.030166\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.021782, dev-loss=-0.004511, mean-recent-eval=0.031610\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.019045, dev-loss=-0.007346, mean-recent-eval=0.033052\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.015591, dev-loss=-0.009743, mean-recent-eval=0.034475\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.012915, dev-loss=-0.009977, mean-recent-eval=0.035863\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.010840, dev-loss=0.003489, mean-recent-eval=0.037206\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.008975, dev-loss=-0.013155, mean-recent-eval=0.038500\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.008842, dev-loss=-0.000427, mean-recent-eval=0.039728\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.006591, dev-loss=-0.004690, mean-recent-eval=0.040889\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.005460, dev-loss=-0.009241, mean-recent-eval=0.041974\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.005597, dev-loss=-0.001774, mean-recent-eval=0.042981\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.004920, dev-loss=-0.010592, mean-recent-eval=0.043906\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.004475, dev-loss=-0.012632, mean-recent-eval=0.044742\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.004087, dev-loss=-0.008911, mean-recent-eval=0.045477\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.004729, dev-loss=0.000339, mean-recent-eval=0.046118\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.005555, dev-loss=0.009576, mean-recent-eval=0.046639\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.004574, dev-loss=0.000435, mean-recent-eval=0.047000\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.003763, dev-loss=-0.005554, mean-recent-eval=0.047238\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.003957, dev-loss=-0.005779, mean-recent-eval=0.047393\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.004779, dev-loss=0.001761, mean-recent-eval=0.047482\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.003702, dev-loss=-0.008210, mean-recent-eval=0.047543\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.004026, dev-loss=-0.001605, mean-recent-eval=0.047580\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.003317, dev-loss=-0.012429, mean-recent-eval=0.047588\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.004402, dev-loss=0.003858, mean-recent-eval=0.047607\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.004276, dev-loss=-0.000181, mean-recent-eval=0.047599\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.003494, dev-loss=-0.014492, mean-recent-eval=0.047622p-value: 0.987654269192708\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9003240226966536\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.00031020421458222727 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0008066321813797494 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0010673870212489197 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.617786, dev-loss=1.642927, mean-recent-eval=-0.128868\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.608729, dev-loss=0.624542, mean-recent-eval=-0.095084\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.081286, dev-loss=0.080300, mean-recent-eval=-0.051644\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.025475, dev-loss=0.022774, mean-recent-eval=-0.022276\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.005352, dev-loss=0.001256, mean-recent-eval=-0.004173\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.003911, dev-loss=-0.000654, mean-recent-eval=0.000313\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.004171, dev-loss=-0.000740, mean-recent-eval=0.000376\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.004486, dev-loss=-0.001399, mean-recent-eval=0.000341\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.004737, dev-loss=-0.001463, mean-recent-eval=0.000316\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.005180, dev-loss=-0.001654, mean-recent-eval=0.000302\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.005333, dev-loss=-0.002025, mean-recent-eval=0.000273\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.005763, dev-loss=-0.002163, mean-recent-eval=0.000247\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.005428, dev-loss=-0.002631, mean-recent-eval=0.000222\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.006056, dev-loss=-0.002963, mean-recent-eval=0.000188\n",
      "best iteration: 460\n",
      "p-value: 0.8262724129579117\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7549826162715724\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0020415072073827374 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.001975426882358911 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.002347097958927592 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.356842, dev-loss=-0.248877, mean-recent-eval=-0.038578\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.083747, dev-loss=0.047761, mean-recent-eval=-0.033014\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.005401, dev-loss=-0.003447, mean-recent-eval=-0.001480\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.001030, dev-loss=0.004317, mean-recent-eval=-0.002324\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.001043, dev-loss=0.000730, mean-recent-eval=-0.002402\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.000755, dev-loss=0.000911, mean-recent-eval=-0.002412\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.000571, dev-loss=0.003486, mean-recent-eval=-0.002344\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.000482, dev-loss=0.002888, mean-recent-eval=-0.002345\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.000704, dev-loss=-0.000196, mean-recent-eval=-0.002287\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000394, dev-loss=-0.000971, mean-recent-eval=-0.002230\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000265, dev-loss=0.001764, mean-recent-eval=-0.002304\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000229, dev-loss=0.001422, mean-recent-eval=-0.002236\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000203, dev-loss=0.002862, mean-recent-eval=-0.002250\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000125, dev-loss=0.000707, mean-recent-eval=-0.002258\n",
      "best iteration: 160\n",
      "p-value: 0.9970464235391108\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8947344246224224\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.009049576048441112 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.008992614503176927 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.005769419151845821 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.036146, dev-loss=-0.041678, mean-recent-eval=-0.237869\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.061565, dev-loss=0.048253, mean-recent-eval=-0.068559\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.020841, dev-loss=0.011528, mean-recent-eval=0.003366\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.016790, dev-loss=0.008280, mean-recent-eval=0.004095\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.013917, dev-loss=0.006802, mean-recent-eval=0.004650\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.010987, dev-loss=0.005929, mean-recent-eval=0.005306\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.007613, dev-loss=0.000928, mean-recent-eval=0.005909\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.007466, dev-loss=0.003926, mean-recent-eval=0.006496\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.005477, dev-loss=-0.001057, mean-recent-eval=0.007081\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.005213, dev-loss=0.001274, mean-recent-eval=0.007625\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004682, dev-loss=-0.000951, mean-recent-eval=0.008091\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.004671, dev-loss=-0.001675, mean-recent-eval=0.008563\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.005215, dev-loss=0.002357, mean-recent-eval=0.008893\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.005086, dev-loss=-0.001550, mean-recent-eval=0.009204\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.005627, dev-loss=0.002736, mean-recent-eval=0.009349\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.005553, dev-loss=-0.000555, mean-recent-eval=0.009503\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.006161, dev-loss=0.004550, mean-recent-eval=0.009578\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.006159, dev-loss=-0.001661, mean-recent-eval=0.009675\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.006339, dev-loss=0.002047, mean-recent-eval=0.009706\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.007010, dev-loss=-0.003302, mean-recent-eval=0.009751\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.006692, dev-loss=0.000019, mean-recent-eval=0.009747\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.006976, dev-loss=0.004688, mean-recent-eval=0.009734\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.007448, dev-loss=-0.001701, mean-recent-eval=0.009801\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.007274, dev-loss=-0.000724, mean-recent-eval=0.009772\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.007311, dev-loss=0.001202, mean-recent-eval=0.009744\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.007526, dev-loss=0.004181, mean-recent-eval=0.009746\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.007915, dev-loss=-0.000350, mean-recent-eval=0.009805\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.007772, dev-loss=0.001640, mean-recent-eval=0.009813\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.007683, dev-loss=0.003319, mean-recent-eval=0.009834\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.008508, dev-loss=-0.001861, mean-recent-eval=0.009847\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.008280, dev-loss=0.004274, mean-recent-eval=0.009843\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.008629, dev-loss=-0.001046, mean-recent-eval=0.009896\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.008283, dev-loss=0.002445, mean-recent-eval=0.009892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.003842, dev-loss=-0.005849, mean-recent-eval=0.047642\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.003403, dev-loss=-0.006222, mean-recent-eval=0.047634\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.003415, dev-loss=-0.008689, mean-recent-eval=0.047578\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.004254, dev-loss=-0.000181, mean-recent-eval=0.047551\n",
      "best iteration: 3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9738824830035783\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8367527485277695\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0005543783576054621 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0005989390793960116 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0007222860306172411 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.041075, dev-loss=0.027373, mean-recent-eval=-0.001746\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.014725, dev-loss=0.009169, mean-recent-eval=-0.000672\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.010527, dev-loss=0.009957, mean-recent-eval=-0.000914\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.008151, dev-loss=0.004691, mean-recent-eval=-0.000797\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.004155, dev-loss=0.005787, mean-recent-eval=-0.000824\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.002541, dev-loss=0.002385, mean-recent-eval=-0.000665\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.002049, dev-loss=-0.005706, mean-recent-eval=-0.000683\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.000954, dev-loss=-0.003018, mean-recent-eval=-0.000701\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.000549, dev-loss=0.003426, mean-recent-eval=-0.000755\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000643, dev-loss=0.006116, mean-recent-eval=-0.000699\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000551, dev-loss=-0.001379, mean-recent-eval=-0.000626\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000560, dev-loss=0.003924, mean-recent-eval=-0.000626\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000574, dev-loss=-0.000701, mean-recent-eval=-0.000636\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000688, dev-loss=0.004627, mean-recent-eval=-0.000700\n",
      "best iteration: 20\n",
      "p-value: 0.9893901304700106\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8857228283934883\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.002429998167820913 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0023473910908371177 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.001992764159616768 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.935474, dev-loss=-0.921321, mean-recent-eval=-0.202977\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.097093, dev-loss=0.092906, mean-recent-eval=-0.130636\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.004431, dev-loss=0.004644, mean-recent-eval=-0.001881\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.003120, dev-loss=0.003252, mean-recent-eval=0.001423\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.001533, dev-loss=0.001267, mean-recent-eval=0.001635\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.001607, dev-loss=0.001746, mean-recent-eval=0.001839\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.000597, dev-loss=0.000464, mean-recent-eval=0.002031\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.000408, dev-loss=-0.000153, mean-recent-eval=0.002204\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.000639, dev-loss=0.000067, mean-recent-eval=0.002331\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000583, dev-loss=0.000055, mean-recent-eval=0.002408\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000818, dev-loss=-0.000572, mean-recent-eval=0.002409\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.001142, dev-loss=-0.000150, mean-recent-eval=0.002408\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001808, dev-loss=-0.000369, mean-recent-eval=0.002422\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.002702, dev-loss=0.000073, mean-recent-eval=0.002405\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.003712, dev-loss=-0.000315, mean-recent-eval=0.002384\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.004617, dev-loss=-0.000166, mean-recent-eval=0.002349\n",
      "best iteration: 1160\n",
      "p-value: 0.952956881551601\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7971010044936492\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.031090261591414182 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.030841149682662335 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02198057315482206 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.272563, dev-loss=-0.316445, mean-recent-eval=-0.067551\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.072148, dev-loss=0.067580, mean-recent-eval=-0.015665\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.062315, dev-loss=0.053610, mean-recent-eval=0.008115\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.056194, dev-loss=0.048909, mean-recent-eval=0.010206\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.050657, dev-loss=0.039718, mean-recent-eval=0.012450\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.038113, dev-loss=0.036770, mean-recent-eval=0.014598\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.029718, dev-loss=0.032363, mean-recent-eval=0.017000\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.019254, dev-loss=0.014761, mean-recent-eval=0.019354\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.013347, dev-loss=0.008350, mean-recent-eval=0.021606\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007829, dev-loss=0.005327, mean-recent-eval=0.023791\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.005760, dev-loss=-0.002702, mean-recent-eval=0.025844\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002453, dev-loss=0.005868, mean-recent-eval=0.027714\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001927, dev-loss=0.004978, mean-recent-eval=0.029405\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001389, dev-loss=0.002429, mean-recent-eval=0.030733\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.001343, dev-loss=-0.001358, mean-recent-eval=0.031864\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.001299, dev-loss=-0.000073, mean-recent-eval=0.032442\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.001643, dev-loss=-0.002216, mean-recent-eval=0.032622\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.001368, dev-loss=0.002245, mean-recent-eval=0.032736\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.001342, dev-loss=0.005541, mean-recent-eval=0.032749\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.001770, dev-loss=-0.001063, mean-recent-eval=0.032571\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.001436, dev-loss=0.006172, mean-recent-eval=0.032719\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.001511, dev-loss=0.003398, mean-recent-eval=0.032656\n",
      "best iteration: 1780\n",
      "\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.008641, dev-loss=0.005091, mean-recent-eval=0.009804\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.009128, dev-loss=-0.002894, mean-recent-eval=0.009836\n",
      "best iteration: 3060\n",
      "p-value: 0.9510298482697954\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.6892209615680379\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.04900298831367744 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.03975479310729221 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02162902546191314 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.430020, dev-loss=-0.404846, mean-recent-eval=-0.183860\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.137014, dev-loss=0.109281, mean-recent-eval=-0.061939\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.041600, dev-loss=0.028913, mean-recent-eval=0.023276\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.036731, dev-loss=0.025134, mean-recent-eval=0.025347\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.034881, dev-loss=0.021664, mean-recent-eval=0.027269\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.031079, dev-loss=0.019737, mean-recent-eval=0.029251\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.026073, dev-loss=0.013950, mean-recent-eval=0.031286\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.022331, dev-loss=0.010817, mean-recent-eval=0.033345\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.017503, dev-loss=0.006070, mean-recent-eval=0.035484\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.014088, dev-loss=0.001723, mean-recent-eval=0.037615\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.014824, dev-loss=0.008196, mean-recent-eval=0.039684\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.010386, dev-loss=0.000354, mean-recent-eval=0.041800\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.008033, dev-loss=-0.002495, mean-recent-eval=0.043833\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.007653, dev-loss=0.000738, mean-recent-eval=0.045820\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.005095, dev-loss=-0.001510, mean-recent-eval=0.047701\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.004933, dev-loss=-0.001763, mean-recent-eval=0.049525\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.004459, dev-loss=0.001122, mean-recent-eval=0.051222\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.004085, dev-loss=0.001220, mean-recent-eval=0.052798\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.003277, dev-loss=0.001140, mean-recent-eval=0.054220\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.002795, dev-loss=-0.000073, mean-recent-eval=0.055529\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.001963, dev-loss=-0.004811, mean-recent-eval=0.056726\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.001916, dev-loss=-0.003033, mean-recent-eval=0.057728\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.001988, dev-loss=-0.001324, mean-recent-eval=0.058604\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.002094, dev-loss=0.000903, mean-recent-eval=0.059297\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.001668, dev-loss=-0.003836, mean-recent-eval=0.059858\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.001815, dev-loss=-0.002884, mean-recent-eval=0.060304\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.001647, dev-loss=-0.004802, mean-recent-eval=0.060671\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.002383, dev-loss=0.002975, mean-recent-eval=0.060909\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.001750, dev-loss=-0.001267, mean-recent-eval=0.061097\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.001963, dev-loss=0.000992, mean-recent-eval=0.061203\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.001806, dev-loss=0.000470, mean-recent-eval=0.061307\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.002075, dev-loss=0.002263, mean-recent-eval=0.061325\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.002009, dev-loss=0.001759, mean-recent-eval=0.061387\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.002019, dev-loss=0.001917, mean-recent-eval=0.061356\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.001680, dev-loss=-0.001011, mean-recent-eval=0.061412\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.002251, dev-loss=0.003581, mean-recent-eval=0.061385\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.001826, dev-loss=-0.006774, mean-recent-eval=0.061340\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.001520, dev-loss=-0.003645, mean-recent-eval=0.061319\n",
      "best iteration: 3340\n",
      "p-value: 0.9798653887639868\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9384796059660298\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.036202512584166055 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.036988376059172066 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02536590177095082 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.337881, dev-loss=-0.300744, mean-recent-eval=-0.015511\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.115204, dev-loss=0.095185, mean-recent-eval=-0.015152\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.069667, dev-loss=0.058014, mean-recent-eval=0.009204\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.068523, dev-loss=0.056081, mean-recent-eval=0.012888\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.064031, dev-loss=0.052631, mean-recent-eval=0.014196\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.059438, dev-loss=0.047955, mean-recent-eval=0.015565\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.054731, dev-loss=0.044450, mean-recent-eval=0.016958\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.049871, dev-loss=0.038913, mean-recent-eval=0.018392\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.043992, dev-loss=0.033822, mean-recent-eval=0.019880\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.039450, dev-loss=0.029118, mean-recent-eval=0.021417\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.033782, dev-loss=0.023834, mean-recent-eval=0.022998\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.028117, dev-loss=0.019323, mean-recent-eval=0.024616\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.023815, dev-loss=0.015953, mean-recent-eval=0.026267\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.020809, dev-loss=0.012722, mean-recent-eval=0.027885\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.016580, dev-loss=0.008978, mean-recent-eval=0.029253\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.013547, dev-loss=0.006295, mean-recent-eval=0.030237\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.011631, dev-loss=0.004363, mean-recent-eval=0.031657\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.010215, dev-loss=0.002624, mean-recent-eval=0.032865\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.008944, dev-loss=0.002085, mean-recent-eval=0.034041\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.008267, dev-loss=0.001243, mean-recent-eval=0.035062\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.008321, dev-loss=0.000748, mean-recent-eval=0.036221\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.007864, dev-loss=0.000453, mean-recent-eval=0.037136\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.008067, dev-loss=0.000220, mean-recent-eval=0.038103\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.007782, dev-loss=-0.000496, mean-recent-eval=0.039049\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.007907, dev-loss=0.000063, mean-recent-eval=0.039724\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.008613, dev-loss=0.000302, mean-recent-eval=0.040167\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.008018, dev-loss=-0.000465, mean-recent-eval=0.040368\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.008316, dev-loss=-0.000142, mean-recent-eval=0.040190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.008588, dev-loss=-0.000436, mean-recent-eval=0.040833\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.008273, dev-loss=-0.000443, mean-recent-eval=0.040722\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.008469, dev-loss=-0.000421, mean-recent-eval=0.040413\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.008462, dev-loss=-0.000426, mean-recent-eval=0.040459\n",
      "best iteration: 2760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9583857261020582\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8789992895895922\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0018222967259547505 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.003482763562217564 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.003466911865742205 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.616575, dev-loss=-0.600690, mean-recent-eval=-0.075143\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.146404, dev-loss=0.133531, mean-recent-eval=-0.087276\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.023714, dev-loss=0.025817, mean-recent-eval=-0.022646\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.004547, dev-loss=0.011144, mean-recent-eval=0.000542\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.006314, dev-loss=0.011304, mean-recent-eval=0.001164\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.008033, dev-loss=0.013200, mean-recent-eval=0.001340\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.009221, dev-loss=0.013843, mean-recent-eval=0.001474\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.011178, dev-loss=0.014145, mean-recent-eval=0.001636\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.012808, dev-loss=0.013446, mean-recent-eval=0.001830\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.014065, dev-loss=0.013029, mean-recent-eval=0.001974\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.014960, dev-loss=0.013074, mean-recent-eval=0.002119\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.015206, dev-loss=0.012882, mean-recent-eval=0.002273\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.014581, dev-loss=0.010211, mean-recent-eval=0.002434\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.015058, dev-loss=0.010772, mean-recent-eval=0.002587\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.014625, dev-loss=0.010074, mean-recent-eval=0.002736\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.014360, dev-loss=0.008860, mean-recent-eval=0.002885\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.013981, dev-loss=0.008589, mean-recent-eval=0.003042\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.013426, dev-loss=0.007398, mean-recent-eval=0.003175\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.013164, dev-loss=0.007509, mean-recent-eval=0.003363\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.012581, dev-loss=0.006028, mean-recent-eval=0.003515\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.012058, dev-loss=0.005441, mean-recent-eval=0.003673\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.011868, dev-loss=0.005554, mean-recent-eval=0.003685\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.011264, dev-loss=0.004942, mean-recent-eval=0.003405\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.010808, dev-loss=0.004512, mean-recent-eval=0.002937\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.010240, dev-loss=0.003258, mean-recent-eval=0.002416\n",
      "best iteration: 2080\n",
      "p-value: 0.996536273734865\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8624569771113557\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.006322561614481088 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.00695799980264003 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.00780819358207132 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.125628, dev-loss=0.041626, mean-recent-eval=0.000125\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.039753, dev-loss=0.061759, mean-recent-eval=-0.002395\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.033497, dev-loss=0.051357, mean-recent-eval=-0.007560\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.025205, dev-loss=0.028673, mean-recent-eval=-0.006685\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.020175, dev-loss=0.030936, mean-recent-eval=-0.006198\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.014531, dev-loss=0.015527, mean-recent-eval=-0.005561\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.009235, dev-loss=0.009852, mean-recent-eval=-0.004987\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.006591, dev-loss=0.014532, mean-recent-eval=-0.004784\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.004225, dev-loss=0.011430, mean-recent-eval=-0.005211\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001680, dev-loss=-0.001301, mean-recent-eval=-0.004711\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.002134, dev-loss=0.014825, mean-recent-eval=-0.004745\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.001741, dev-loss=0.005289, mean-recent-eval=-0.005269\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001596, dev-loss=-0.001524, mean-recent-eval=-0.005349\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001807, dev-loss=0.001828, mean-recent-eval=-0.005161\n",
      "best iteration: 40\n",
      "p-value: 0.7823677379765795\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.6513319078227455\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.013619716263221586 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.013837319541472154 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.011940159942282239 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.066077, dev-loss=0.073044, mean-recent-eval=-0.013955\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.079802, dev-loss=0.088064, mean-recent-eval=-0.001803\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.054289, dev-loss=0.050582, mean-recent-eval=0.003283\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.050368, dev-loss=0.046232, mean-recent-eval=0.004844\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.048319, dev-loss=0.045265, mean-recent-eval=0.005427\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.044144, dev-loss=0.039929, mean-recent-eval=0.006076\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.041268, dev-loss=0.038108, mean-recent-eval=0.006710\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.036810, dev-loss=0.032655, mean-recent-eval=0.007212\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.033614, dev-loss=0.030220, mean-recent-eval=0.007696\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.029436, dev-loss=0.026677, mean-recent-eval=0.008198\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.026568, dev-loss=0.022747, mean-recent-eval=0.008712\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.024706, dev-loss=0.024255, mean-recent-eval=0.009237\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.021049, dev-loss=0.017345, mean-recent-eval=0.009769\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.020011, dev-loss=0.018545, mean-recent-eval=0.010296\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.017986, dev-loss=0.016386, mean-recent-eval=0.010818\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.016259, dev-loss=0.014086, mean-recent-eval=0.011335\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.015369, dev-loss=0.013298, mean-recent-eval=0.011838\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.014202, dev-loss=0.006988, mean-recent-eval=0.012321\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.013858, dev-loss=0.006803, mean-recent-eval=0.012784\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.012563, dev-loss=0.008321, mean-recent-eval=0.013227\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.012160, dev-loss=0.008242, mean-recent-eval=0.013629\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.012115, dev-loss=0.009545, mean-recent-eval=0.014000\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.011820, dev-loss=0.010146, mean-recent-eval=0.014320p-value: 0.9516187994628654\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8236599048007927\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0012533360580700842 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0010975876223297835 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0010946402228719006 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.858879, dev-loss=0.899994, mean-recent-eval=-0.234954\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.036923, dev-loss=0.042082, mean-recent-eval=-0.081044\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.003931, dev-loss=0.009095, mean-recent-eval=0.000211\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.003402, dev-loss=0.005975, mean-recent-eval=0.000987\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.003076, dev-loss=0.005854, mean-recent-eval=0.000918\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.002965, dev-loss=0.005096, mean-recent-eval=0.000993\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.002369, dev-loss=0.004237, mean-recent-eval=0.001081\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.002284, dev-loss=0.002300, mean-recent-eval=0.001039\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.002325, dev-loss=0.004899, mean-recent-eval=0.000975\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.002036, dev-loss=0.003070, mean-recent-eval=0.001052\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.002052, dev-loss=0.003476, mean-recent-eval=0.000998\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002201, dev-loss=0.000008, mean-recent-eval=0.000965\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002488, dev-loss=0.005220, mean-recent-eval=0.000980\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001971, dev-loss=0.002069, mean-recent-eval=0.000967\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.002256, dev-loss=0.000955, mean-recent-eval=0.000949\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.002471, dev-loss=0.000326, mean-recent-eval=0.000903\n",
      "best iteration: 1100\n",
      "p-value: 0.9675718313321447\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.728844198999427\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.017095402152496934 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.01712709079105847 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.012705615511135623 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.472539, dev-loss=0.510907, mean-recent-eval=-0.262612\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.408420, dev-loss=0.456029, mean-recent-eval=-0.211168\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.199698, dev-loss=0.229452, mean-recent-eval=-0.128651\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.081358, dev-loss=0.091894, mean-recent-eval=-0.053674\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.038478, dev-loss=0.026988, mean-recent-eval=-0.000958\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.037695, dev-loss=0.027492, mean-recent-eval=0.004052\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.035089, dev-loss=0.024138, mean-recent-eval=0.004625\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.032251, dev-loss=0.019443, mean-recent-eval=0.005222\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.029951, dev-loss=0.019357, mean-recent-eval=0.005796\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.026393, dev-loss=0.015575, mean-recent-eval=0.006429\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.024214, dev-loss=0.011536, mean-recent-eval=0.007067\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.021818, dev-loss=0.010722, mean-recent-eval=0.007718\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.019425, dev-loss=0.009033, mean-recent-eval=0.008380\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.016424, dev-loss=0.010733, mean-recent-eval=0.009088\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.013572, dev-loss=0.001821, mean-recent-eval=0.009774\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.010897, dev-loss=0.001737, mean-recent-eval=0.010484\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.010166, dev-loss=0.001886, mean-recent-eval=0.011201\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.007501, dev-loss=-0.000372, mean-recent-eval=0.011902\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.005918, dev-loss=0.001361, mean-recent-eval=0.012595\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.004286, dev-loss=-0.002146, mean-recent-eval=0.013293\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.003170, dev-loss=-0.000059, mean-recent-eval=0.013937\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.002269, dev-loss=0.001358, mean-recent-eval=0.014611\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.001573, dev-loss=-0.004513, mean-recent-eval=0.015191\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.001089, dev-loss=-0.000422, mean-recent-eval=0.015771\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000743, dev-loss=0.000431, mean-recent-eval=0.016314\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000528, dev-loss=-0.000088, mean-recent-eval=0.016846\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000296, dev-loss=0.001046, mean-recent-eval=0.017262\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000304, dev-loss=0.000892, mean-recent-eval=0.017678\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000392, dev-loss=0.003571, mean-recent-eval=0.017997\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000596, dev-loss=-0.002746, mean-recent-eval=0.018108\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000379, dev-loss=0.003029, mean-recent-eval=0.018132\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000608, dev-loss=-0.003032, mean-recent-eval=0.018130\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.000573, dev-loss=-0.000135, mean-recent-eval=0.018144\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.000625, dev-loss=-0.000016, mean-recent-eval=0.018135\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.000753, dev-loss=-0.001942, mean-recent-eval=0.018091\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.000823, dev-loss=-0.000984, mean-recent-eval=0.018091\n",
      "best iteration: 3160\n",
      "p-value: 0.8629379954971461\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7977807252207377\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.019331201490665553 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.019088050151822532 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.01462166938482517 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-2.230889, dev-loss=-2.389982, mean-recent-eval=-0.035692\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.389221, dev-loss=0.393531, mean-recent-eval=-0.145841\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.068320, dev-loss=0.061276, mean-recent-eval=-0.024987\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.044616, dev-loss=0.032663, mean-recent-eval=0.006263\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.034935, dev-loss=0.024823, mean-recent-eval=0.007633\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.029810, dev-loss=0.021698, mean-recent-eval=0.009050\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.021638, dev-loss=0.013409, mean-recent-eval=0.010494\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.013867, dev-loss=0.002716, mean-recent-eval=0.011940\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.009606, dev-loss=0.001127, mean-recent-eval=0.013393\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.005773, dev-loss=-0.002098, mean-recent-eval=0.014775\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.011845, dev-loss=0.007609, mean-recent-eval=0.014522\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.011388, dev-loss=0.010395, mean-recent-eval=0.014648\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.011654, dev-loss=0.008666, mean-recent-eval=0.014733\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.011520, dev-loss=0.008251, mean-recent-eval=0.014758\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.011105, dev-loss=0.010229, mean-recent-eval=0.014810\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.012218, dev-loss=0.004966, mean-recent-eval=0.014831\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.011393, dev-loss=0.008747, mean-recent-eval=0.014873\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.011334, dev-loss=0.009456, mean-recent-eval=0.014905\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.011415, dev-loss=0.008305, mean-recent-eval=0.014951\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.011504, dev-loss=0.006647, mean-recent-eval=0.014962\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.011590, dev-loss=0.006889, mean-recent-eval=0.014967\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.011480, dev-loss=0.007826, mean-recent-eval=0.014981\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.011537, dev-loss=0.006255, mean-recent-eval=0.015002\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.011319, dev-loss=0.007829, mean-recent-eval=0.015009\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.011381, dev-loss=0.007934, mean-recent-eval=0.015030\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.011293, dev-loss=0.007863, mean-recent-eval=0.015041\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.011535, dev-loss=0.006470, mean-recent-eval=0.015067\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.011314, dev-loss=0.008584, mean-recent-eval=0.015079\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.011147, dev-loss=0.010846, mean-recent-eval=0.015128\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.011393, dev-loss=0.007655, mean-recent-eval=0.015143\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.011288, dev-loss=0.008231, mean-recent-eval=0.015168\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.010954, dev-loss=0.009944, mean-recent-eval=0.015180\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.011362, dev-loss=0.006763, mean-recent-eval=0.015212\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.011480, dev-loss=0.006740, mean-recent-eval=0.015209\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.011453, dev-loss=0.007449, mean-recent-eval=0.015200\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.011263, dev-loss=0.009373, mean-recent-eval=0.015225\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.011385, dev-loss=0.006974, mean-recent-eval=0.015231\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=0.011545, dev-loss=0.007074, mean-recent-eval=0.015246\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.011614, dev-loss=0.006140, mean-recent-eval=0.015250\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=0.011175, dev-loss=0.009331, mean-recent-eval=0.015270\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=0.011475, dev-loss=0.007911, mean-recent-eval=0.015332\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.011398, dev-loss=0.007935, mean-recent-eval=0.015345\n",
      "iteration 5500, dev-MSE=0.000000, train-loss=0.011495, dev-loss=0.006518, mean-recent-eval=0.015346\n",
      "iteration 5600, dev-MSE=0.000000, train-loss=0.011446, dev-loss=0.007118, mean-recent-eval=0.015399\n",
      "iteration 5700, dev-MSE=0.000000, train-loss=0.011820, dev-loss=0.005877, mean-recent-eval=0.015420\n",
      "iteration 5800, dev-MSE=0.000000, train-loss=0.011464, dev-loss=0.007539, mean-recent-eval=0.015433\n",
      "iteration 5900, dev-MSE=0.000000, train-loss=0.011503, dev-loss=0.006525, mean-recent-eval=0.015419\n",
      "best iteration: 5760\n",
      "\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.003550, dev-loss=-0.000806, mean-recent-eval=0.016089\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002932, dev-loss=0.001314, mean-recent-eval=0.017286\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001858, dev-loss=-0.002021, mean-recent-eval=0.018352\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001261, dev-loss=-0.010226, mean-recent-eval=0.019238\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.001227, dev-loss=-0.005079, mean-recent-eval=0.019808\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.001604, dev-loss=-0.000111, mean-recent-eval=0.020014\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.001586, dev-loss=0.000304, mean-recent-eval=0.020058\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.001795, dev-loss=0.001793, mean-recent-eval=0.020036\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.001763, dev-loss=0.001285, mean-recent-eval=0.020055\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.001173, dev-loss=-0.003874, mean-recent-eval=0.020057\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.001318, dev-loss=-0.002111, mean-recent-eval=0.020096\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.001103, dev-loss=-0.006809, mean-recent-eval=0.020136\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.001180, dev-loss=-0.004421, mean-recent-eval=0.020180\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.001829, dev-loss=0.001320, mean-recent-eval=0.020229\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000967, dev-loss=-0.009045, mean-recent-eval=0.020140\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.001046, dev-loss=-0.006494, mean-recent-eval=0.020134\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.001288, dev-loss=-0.002417, mean-recent-eval=0.020179\n",
      "best iteration: 2280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "IPU available: False, using: 0 IPUs\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9935162281193584\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9050864775532997\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.03202152125471611 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.03130912863828343 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.020462903033106297 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-2.206342, dev-loss=-2.001995, mean-recent-eval=-0.175585\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.350049, dev-loss=0.299371, mean-recent-eval=-0.193315\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.077495, dev-loss=0.043871, mean-recent-eval=-0.000668\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.056707, dev-loss=0.033644, mean-recent-eval=0.009624\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.050471, dev-loss=0.026624, mean-recent-eval=0.011520\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.039398, dev-loss=0.015713, mean-recent-eval=0.013697\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.027977, dev-loss=0.011361, mean-recent-eval=0.015897\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.020202, dev-loss=0.005934, mean-recent-eval=0.018303\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.016553, dev-loss=0.003803, mean-recent-eval=0.020728\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007350, dev-loss=-0.002357, mean-recent-eval=0.023155\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004752, dev-loss=-0.001858, mean-recent-eval=0.025489\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002964, dev-loss=-0.002289, mean-recent-eval=0.027708\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001376, dev-loss=-0.001805, mean-recent-eval=0.029687\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000905, dev-loss=-0.002298, mean-recent-eval=0.031415\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000741, dev-loss=-0.001396, mean-recent-eval=0.032824\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000720, dev-loss=-0.000904, mean-recent-eval=0.033588\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000824, dev-loss=0.000796, mean-recent-eval=0.033739\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000807, dev-loss=0.000383, mean-recent-eval=0.033749\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000497, dev-loss=-0.001230, mean-recent-eval=0.033709\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000637, dev-loss=0.000043, mean-recent-eval=0.033681\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000561, dev-loss=0.000390, mean-recent-eval=0.033818\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000613, dev-loss=-0.000947, mean-recent-eval=0.033860\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000472, dev-loss=0.000215, mean-recent-eval=0.033872\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000513, dev-loss=-0.000071, mean-recent-eval=0.033747\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000794, dev-loss=-0.000717, mean-recent-eval=0.033744\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000785, dev-loss=0.000824, mean-recent-eval=0.033746\n",
      "best iteration: 2140\n",
      "p-value: 0.8626967505398592\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.6298997199243058\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.008788645952648267 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.008751836633150668 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.008031915749268748 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.362514, dev-loss=-0.407889, mean-recent-eval=-0.016464\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.073647, dev-loss=0.103974, mean-recent-eval=-0.010928\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.057163, dev-loss=0.079416, mean-recent-eval=0.003289\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.044240, dev-loss=0.060592, mean-recent-eval=0.004168\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.032705, dev-loss=0.047135, mean-recent-eval=0.004988\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.020549, dev-loss=0.026930, mean-recent-eval=0.005823\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.012663, dev-loss=0.017512, mean-recent-eval=0.006662\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.005165, dev-loss=0.014576, mean-recent-eval=0.007415\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.002289, dev-loss=0.001847, mean-recent-eval=0.008097\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000844, dev-loss=0.002475, mean-recent-eval=0.008727\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.001071, dev-loss=0.008645, mean-recent-eval=0.009082\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.001038, dev-loss=0.001889, mean-recent-eval=0.009162\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001724, dev-loss=0.011390, mean-recent-eval=0.009183\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001600, dev-loss=0.001315, mean-recent-eval=0.009183\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.001790, dev-loss=0.003018, mean-recent-eval=0.009185\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.002094, dev-loss=0.001121, mean-recent-eval=0.009208\n",
      "best iteration: 1160\n",
      "p-value: 0.947465773822132\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9108864543663168\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.07078870310783053 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.013374402751547124 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.010135016836520285 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.799303, dev-loss=-0.837497, mean-recent-eval=-0.587682\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.814320, dev-loss=0.796352, mean-recent-eval=-0.571585\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.737789, dev-loss=0.737156, mean-recent-eval=-0.494429\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.638225, dev-loss=0.644655, mean-recent-eval=-0.421447\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.526749, dev-loss=0.534250, mean-recent-eval=-0.349710\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.437319, dev-loss=0.445646, mean-recent-eval=-0.278608\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.347727, dev-loss=0.356653, mean-recent-eval=-0.208193\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.266086, dev-loss=0.274822, mean-recent-eval=-0.140214\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.199044, dev-loss=0.207283, mean-recent-eval=-0.075977\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.155143, dev-loss=0.162609, mean-recent-eval=-0.034071\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.133839, dev-loss=0.140588, mean-recent-eval=-0.003028\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.122587, dev-loss=0.128190, mean-recent-eval=0.009669\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.119851, dev-loss=0.125386, mean-recent-eval=0.007661\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.113070, dev-loss=0.118000, mean-recent-eval=0.005069\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.108495, dev-loss=0.112870, mean-recent-eval=0.002403\n",
      "best iteration: 1040\n",
      "p-value: 0.9751965407739551\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8102334494168973\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.012515539613695115 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.04477035413847604 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.038478436777216005 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.398496, dev-loss=-0.418582, mean-recent-eval=-0.135676\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.167607, dev-loss=0.181509, mean-recent-eval=-0.098879\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.127562, dev-loss=0.132621, mean-recent-eval=-0.012134\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.116819, dev-loss=0.118638, mean-recent-eval=0.013567\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.110045, dev-loss=0.113354, mean-recent-eval=0.015921\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.100314, dev-loss=0.102590, mean-recent-eval=0.017858\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.094674, dev-loss=0.097941, mean-recent-eval=0.019837\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.084068, dev-loss=0.086671, mean-recent-eval=0.021895\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.075065, dev-loss=0.078296, mean-recent-eval=0.024044\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.065312, dev-loss=0.067729, mean-recent-eval=0.026292\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.056845, dev-loss=0.059099, mean-recent-eval=0.028627\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.048956, dev-loss=0.050677, mean-recent-eval=0.031044\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.041038, dev-loss=0.041964, mean-recent-eval=0.033558\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.033397, dev-loss=0.034735, mean-recent-eval=0.036117\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.027145, dev-loss=0.028733, mean-recent-eval=0.038739\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.022272, dev-loss=0.022693, mean-recent-eval=0.041384\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.018863, dev-loss=0.019345, mean-recent-eval=0.043973\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.016830, dev-loss=0.016766, mean-recent-eval=0.046498\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.016028, dev-loss=0.015806, mean-recent-eval=0.048951\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.016005, dev-loss=0.016067, mean-recent-eval=0.051214\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.015410, dev-loss=0.015858, mean-recent-eval=0.053297\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.015570, dev-loss=0.015716, mean-recent-eval=0.055124\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.015757, dev-loss=0.015634, mean-recent-eval=0.056327\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.015479, dev-loss=0.015973, mean-recent-eval=0.056977\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.015608, dev-loss=0.015491, mean-recent-eval=0.057327\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.015443, dev-loss=0.015495, mean-recent-eval=0.057348\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.015428, dev-loss=0.015384, mean-recent-eval=0.057376\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.015425, dev-loss=0.015290, mean-recent-eval=0.057413\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.015450, dev-loss=0.015210, mean-recent-eval=0.057541\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.015345, dev-loss=0.015142, mean-recent-eval=0.057701\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.015336, dev-loss=0.014985, mean-recent-eval=0.057634\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.015355, dev-loss=0.014981, mean-recent-eval=0.057815\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.015337, dev-loss=0.014972, mean-recent-eval=0.057883\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.015171, dev-loss=0.014760, mean-recent-eval=0.057950\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.014858, dev-loss=0.014668, mean-recent-eval=0.058123\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.014726, dev-loss=0.014575, mean-recent-eval=0.058119\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.014556, dev-loss=0.014515, mean-recent-eval=0.058198\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.014375, dev-loss=0.014558, mean-recent-eval=0.058364\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.014134, dev-loss=0.014659, mean-recent-eval=0.058342\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.014350, dev-loss=0.014380, mean-recent-eval=0.058482\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.014349, dev-loss=0.014302, mean-recent-eval=0.058388\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.014114, dev-loss=0.014424, mean-recent-eval=0.058526\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.014306, dev-loss=0.014131, mean-recent-eval=0.058559\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.014033, dev-loss=0.014158, mean-recent-eval=0.058569\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.013990, dev-loss=0.014057, mean-recent-eval=0.058691\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.013552, dev-loss=0.014092, mean-recent-eval=0.058824\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.013765, dev-loss=0.013878, mean-recent-eval=0.058815\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.013785, dev-loss=0.013735, mean-recent-eval=0.058861\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.013704, dev-loss=0.013738, mean-recent-eval=0.058848\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.013998, dev-loss=0.013645, mean-recent-eval=0.058908\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=0.013527, dev-loss=0.013631, mean-recent-eval=0.059187\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.013645, dev-loss=0.013351, mean-recent-eval=0.059063\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=0.013506, dev-loss=0.013368, mean-recent-eval=0.059132\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=0.013320, dev-loss=0.013414, mean-recent-eval=0.059256\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.013302, dev-loss=0.013237, mean-recent-eval=0.059322\n",
      "iteration 5500, dev-MSE=0.000000, train-loss=0.013483, dev-loss=0.013143, mean-recent-eval=0.059416\n",
      "iteration 5600, dev-MSE=0.000000, train-loss=0.013249, dev-loss=0.013067, mean-recent-eval=0.059491\n",
      "iteration 5700, dev-MSE=0.000000, train-loss=0.013093, dev-loss=0.013053, mean-recent-eval=0.059531\n",
      "iteration 5800, dev-MSE=0.000000, train-loss=0.013162, dev-loss=0.012887, mean-recent-eval=0.059411\n",
      "iteration 5900, dev-MSE=0.000000, train-loss=0.012832, dev-loss=0.012877, mean-recent-eval=0.059410\n",
      "best iteration: 5560\n",
      "p-value: 0.7941286324960296\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8287442000971517\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.045133675820595714 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0124082390910287 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.012339499873598395 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.478389, dev-loss=-0.554916, mean-recent-eval=-0.065818\n",
      "iteration 100, dev-MSE=0.000000, train-loss=-0.026244, dev-loss=0.000605, mean-recent-eval=-0.081031\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.050155, dev-loss=0.065008, mean-recent-eval=-0.043889\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.120211, dev-loss=0.119442, mean-recent-eval=-0.004002\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.115375, dev-loss=0.108056, mean-recent-eval=0.004448\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.111884, dev-loss=0.106071, mean-recent-eval=0.005465\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.108232, dev-loss=0.100109, mean-recent-eval=0.006293\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.103935, dev-loss=0.097532, mean-recent-eval=0.007093\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.098352, dev-loss=0.094075, mean-recent-eval=0.007947\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.093812, dev-loss=0.088201, mean-recent-eval=0.008787\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.091225, dev-loss=0.086152, mean-recent-eval=0.009612\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.085896, dev-loss=0.081969, mean-recent-eval=0.010229\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.079178, dev-loss=0.073500, mean-recent-eval=0.008595\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.076048, dev-loss=0.071889, mean-recent-eval=0.006397\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.071629, dev-loss=0.066866, mean-recent-eval=0.004222\n",
      "best iteration: 1060\n",
      "p-value: 0.9481265449839029\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.876287566422057\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.05440842073155181 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.05392716604243223 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.04197126537433782 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.760825, dev-loss=0.656137, mean-recent-eval=-0.113214\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.163485, dev-loss=0.112139, mean-recent-eval=-0.023359\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.130500, dev-loss=0.088574, mean-recent-eval=0.015561\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.114937, dev-loss=0.071698, mean-recent-eval=0.019796\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.084567, dev-loss=0.048906, mean-recent-eval=0.024670\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.054227, dev-loss=0.026770, mean-recent-eval=0.030055\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.029600, dev-loss=0.013366, mean-recent-eval=0.035702\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.014878, dev-loss=0.000194, mean-recent-eval=0.041406\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.005853, dev-loss=-0.000281, mean-recent-eval=0.046675\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001613, dev-loss=-0.001122, mean-recent-eval=0.051189\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000590, dev-loss=0.001804, mean-recent-eval=0.054579\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000869, dev-loss=-0.000441, mean-recent-eval=0.056299\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001308, dev-loss=0.000100, mean-recent-eval=0.056552\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001014, dev-loss=0.003775, mean-recent-eval=0.056621\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.001014, dev-loss=0.003167, mean-recent-eval=0.056625\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.001004, dev-loss=0.001548, mean-recent-eval=0.056549\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.001293, dev-loss=0.001926, mean-recent-eval=0.056611\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.002092, dev-loss=0.005837, mean-recent-eval=0.056704\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.001635, dev-loss=0.004753, mean-recent-eval=0.056558\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.001314, dev-loss=0.003501, mean-recent-eval=0.056815\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.001545, dev-loss=0.003527, mean-recent-eval=0.056716\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.001757, dev-loss=0.005007, mean-recent-eval=0.056750\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.001847, dev-loss=0.002898, mean-recent-eval=0.056652\n",
      "best iteration: 1880\n",
      "p-value: 0.9743492531932657\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.847234650445255\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.027636964073267378 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02716345950409809 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.023522924972070182 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.051660, dev-loss=-0.095553, mean-recent-eval=-0.030583\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.143260, dev-loss=0.139394, mean-recent-eval=0.000755\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.121161, dev-loss=0.107608, mean-recent-eval=0.006021\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.097875, dev-loss=0.097602, mean-recent-eval=0.008672\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.072150, dev-loss=0.075768, mean-recent-eval=0.011278\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.049057, dev-loss=0.050787, mean-recent-eval=0.014026\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.029479, dev-loss=0.022474, mean-recent-eval=0.016879\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.015093, dev-loss=0.015661, mean-recent-eval=0.019717\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.007600, dev-loss=0.009262, mean-recent-eval=0.022398\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.004488, dev-loss=0.006683, mean-recent-eval=0.024803\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004778, dev-loss=-0.003383, mean-recent-eval=0.026776\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.005662, dev-loss=-0.003079, mean-recent-eval=0.028001\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.005282, dev-loss=0.001545, mean-recent-eval=0.028187\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.005427, dev-loss=0.007409, mean-recent-eval=0.028173\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.005387, dev-loss=0.011647, mean-recent-eval=0.028153\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.005715, dev-loss=0.013141, mean-recent-eval=0.028091\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.005753, dev-loss=0.011711, mean-recent-eval=0.028212\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.006857, dev-loss=0.003953, mean-recent-eval=0.028295\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.006350, dev-loss=0.006305, mean-recent-eval=0.028393\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.005775, dev-loss=0.012022, mean-recent-eval=0.028370\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.005910, dev-loss=0.009116, mean-recent-eval=0.028399\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.006054, dev-loss=0.006844, mean-recent-eval=0.028370\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.006064, dev-loss=0.006630, mean-recent-eval=0.028419\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.005526, dev-loss=0.009886, mean-recent-eval=0.028540\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.005191, dev-loss=0.018254, mean-recent-eval=0.028599\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.005256, dev-loss=0.010781, mean-recent-eval=0.028617\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.006658, dev-loss=0.001505, mean-recent-eval=0.028778\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.004901, dev-loss=0.018671, mean-recent-eval=0.028793\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.004874, dev-loss=0.013551, mean-recent-eval=0.028850\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.004729, dev-loss=0.016978, mean-recent-eval=0.028937\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.004900, dev-loss=0.009211, mean-recent-eval=0.028955\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.004519, dev-loss=0.013254, mean-recent-eval=0.028998\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.004900, dev-loss=0.006341, mean-recent-eval=0.029120\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.005155, dev-loss=0.003786, mean-recent-eval=0.029134\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.005836, dev-loss=-0.001315, mean-recent-eval=0.029175\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.004213, dev-loss=0.011918, mean-recent-eval=0.029268\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.004127, dev-loss=0.013415, mean-recent-eval=0.029345\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.004216, dev-loss=0.010180, mean-recent-eval=0.029343\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.004265, dev-loss=0.006262, mean-recent-eval=0.029380\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.004958, dev-loss=0.001529, mean-recent-eval=0.029512\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.004013, dev-loss=0.008031, mean-recent-eval=0.029594\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.004475, dev-loss=0.001591, mean-recent-eval=0.029590\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.003690, dev-loss=0.015965, mean-recent-eval=0.029647\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.003743, dev-loss=0.009878, mean-recent-eval=0.029707\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.004884, dev-loss=-0.002566, mean-recent-eval=0.029805\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.003787, dev-loss=0.006307, mean-recent-eval=0.029822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.7359668691807557\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.6355771383948758\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.020969233584873133 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.020845683242439964 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.01608064728297983 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.422914, dev-loss=0.421512, mean-recent-eval=-0.032962\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.060912, dev-loss=0.042246, mean-recent-eval=0.001107\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.055069, dev-loss=0.035726, mean-recent-eval=0.004822\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.049387, dev-loss=0.026690, mean-recent-eval=0.006071\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.044550, dev-loss=0.021446, mean-recent-eval=0.007394\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.037214, dev-loss=0.017480, mean-recent-eval=0.008846\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.029481, dev-loss=0.012523, mean-recent-eval=0.010395\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.019471, dev-loss=0.003973, mean-recent-eval=0.011993\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.013035, dev-loss=0.000691, mean-recent-eval=0.013670\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.008416, dev-loss=-0.000989, mean-recent-eval=0.015268\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004183, dev-loss=-0.001511, mean-recent-eval=0.016912\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.003544, dev-loss=-0.001484, mean-recent-eval=0.018338\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002007, dev-loss=-0.001056, mean-recent-eval=0.019663\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001135, dev-loss=-0.000809, mean-recent-eval=0.020783\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000684, dev-loss=0.001655, mean-recent-eval=0.021619\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000749, dev-loss=0.001058, mean-recent-eval=0.022055\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000749, dev-loss=0.001421, mean-recent-eval=0.022222\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.001078, dev-loss=0.002639, mean-recent-eval=0.022425\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000715, dev-loss=0.002421, mean-recent-eval=0.022347\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000699, dev-loss=0.000889, mean-recent-eval=0.022271\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.001586, dev-loss=0.001987, mean-recent-eval=0.022369\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.001146, dev-loss=0.000589, mean-recent-eval=0.022442\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.001108, dev-loss=0.002256, mean-recent-eval=0.022384\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.001195, dev-loss=0.001861, mean-recent-eval=0.022252\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.001352, dev-loss=0.000733, mean-recent-eval=0.022306\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.001613, dev-loss=0.001854, mean-recent-eval=0.022311\n",
      "best iteration: 2100\n",
      "p-value: 0.8015611390609735\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.710314554741626\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.03728366192726689 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.005668546022318088 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0004076424908539232 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.496048, dev-loss=-0.253067, mean-recent-eval=-0.002934\n",
      "iteration 100, dev-MSE=0.000000, train-loss=-0.184435, dev-loss=-0.041821, mean-recent-eval=-0.014524\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.178570, dev-loss=0.199948, mean-recent-eval=-0.038257\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.121632, dev-loss=0.151407, mean-recent-eval=-0.008840\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.091317, dev-loss=0.132375, mean-recent-eval=-0.001376\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.080225, dev-loss=0.127843, mean-recent-eval=-0.000153\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.077671, dev-loss=0.125637, mean-recent-eval=-0.000465\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.074397, dev-loss=0.121864, mean-recent-eval=-0.000474\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.070505, dev-loss=0.118827, mean-recent-eval=-0.000478\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.068754, dev-loss=0.116394, mean-recent-eval=-0.000482\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.064310, dev-loss=0.110851, mean-recent-eval=-0.000479\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.061808, dev-loss=0.107082, mean-recent-eval=-0.000486\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.058515, dev-loss=0.103401, mean-recent-eval=-0.000475\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.055248, dev-loss=0.097036, mean-recent-eval=-0.000480\n",
      "best iteration: 440\n",
      "p-value: 0.9817301036333145\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7939337978567731\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.03295821363044405 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.03270593391287894 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02709374509184449 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.157574, dev-loss=1.393422, mean-recent-eval=-0.057929\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.121251, dev-loss=0.102489, mean-recent-eval=-0.004948\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.104348, dev-loss=0.078250, mean-recent-eval=0.008875\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.080015, dev-loss=0.062519, mean-recent-eval=0.012060\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.053825, dev-loss=0.030035, mean-recent-eval=0.015497\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.033797, dev-loss=0.019600, mean-recent-eval=0.019055\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.019852, dev-loss=0.000209, mean-recent-eval=0.022689\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.007941, dev-loss=0.002404, mean-recent-eval=0.026143\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.004186, dev-loss=-0.008728, mean-recent-eval=0.029347\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.003342, dev-loss=-0.013041, mean-recent-eval=0.031968\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.002457, dev-loss=0.008460, mean-recent-eval=0.033527\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.003699, dev-loss=-0.003610, mean-recent-eval=0.033784\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.004922, dev-loss=-0.002465, mean-recent-eval=0.033567\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.006298, dev-loss=-0.001165, mean-recent-eval=0.033374\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.007426, dev-loss=0.006202, mean-recent-eval=0.033061\n",
      "best iteration: 1040\n",
      "p-value: 0.9903221311372707\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8997559645469004\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.013369592203819263 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.013346444494887923 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.011752703460161175 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-2.580723, dev-loss=-2.452240, mean-recent-eval=-0.184414\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.433758, dev-loss=0.403984, mean-recent-eval=-0.215391\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.195730, dev-loss=0.182571, mean-recent-eval=-0.033682\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.174408, dev-loss=0.164909, mean-recent-eval=0.001404\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.138548, dev-loss=0.130466, mean-recent-eval=0.004103\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.094167, dev-loss=0.086643, mean-recent-eval=0.006114\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.049158, dev-loss=0.043418, mean-recent-eval=0.007986\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.019921, dev-loss=0.015281, mean-recent-eval=0.009839\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.005301, dev-loss=0.002677, mean-recent-eval=0.011418\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000537, dev-loss=-0.002106, mean-recent-eval=0.012410\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000350, dev-loss=0.002861, mean-recent-eval=0.013276\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000691, dev-loss=0.004323, mean-recent-eval=0.013370\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001307, dev-loss=-0.002481, mean-recent-eval=0.013390\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001683, dev-loss=0.003902, mean-recent-eval=0.013354\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.003335, dev-loss=-0.001569, mean-recent-eval=0.013303\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.003700, dev-loss=0.009302, mean-recent-eval=0.013373\n",
      "best iteration: 1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.004579, dev-loss=0.000528, mean-recent-eval=0.029885\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.003621, dev-loss=0.010166, mean-recent-eval=0.029950\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.004087, dev-loss=0.002427, mean-recent-eval=0.030023\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.003658, dev-loss=0.005290, mean-recent-eval=0.030034\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=0.003237, dev-loss=0.012285, mean-recent-eval=0.030135\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.003356, dev-loss=0.007131, mean-recent-eval=0.030096\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=0.003910, dev-loss=0.001006, mean-recent-eval=0.030207\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=0.003224, dev-loss=0.006337, mean-recent-eval=0.030240\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.003139, dev-loss=0.010489, mean-recent-eval=0.030211\n",
      "iteration 5500, dev-MSE=0.000000, train-loss=0.004632, dev-loss=-0.003605, mean-recent-eval=0.030353\n",
      "iteration 5600, dev-MSE=0.000000, train-loss=0.003316, dev-loss=0.003369, mean-recent-eval=0.030383\n",
      "iteration 5700, dev-MSE=0.000000, train-loss=0.003143, dev-loss=0.004179, mean-recent-eval=0.030401\n",
      "iteration 5800, dev-MSE=0.000000, train-loss=0.002782, dev-loss=0.009635, mean-recent-eval=0.030488\n",
      "iteration 5900, dev-MSE=0.000000, train-loss=0.003072, dev-loss=0.005132, mean-recent-eval=0.030474\n",
      "best iteration: 5960\n",
      "p-value: 0.9662085364021148\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8736302595452303\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.04613376759138743 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.042730135849020065 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.03656415771511104 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.549576, dev-loss=-0.736305, mean-recent-eval=-0.012920\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.190226, dev-loss=0.207790, mean-recent-eval=-0.022290\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.155650, dev-loss=0.157081, mean-recent-eval=0.016214\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.135429, dev-loss=0.131894, mean-recent-eval=0.019268\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.104762, dev-loss=0.098709, mean-recent-eval=0.022458\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.071045, dev-loss=0.059510, mean-recent-eval=0.026087\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.044209, dev-loss=0.037508, mean-recent-eval=0.030102\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.024242, dev-loss=0.016890, mean-recent-eval=0.034281\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.013151, dev-loss=0.007733, mean-recent-eval=0.038313\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.011710, dev-loss=0.003643, mean-recent-eval=0.041825\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.012588, dev-loss=0.009010, mean-recent-eval=0.044431\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.014944, dev-loss=0.006281, mean-recent-eval=0.045301\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.016192, dev-loss=0.010158, mean-recent-eval=0.045157\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.017343, dev-loss=0.008919, mean-recent-eval=0.045132\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.017945, dev-loss=0.008114, mean-recent-eval=0.045158\n",
      "best iteration: 1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py:1784: UserWarning: You passed a edgecolor/edgecolors ((1.0, 0.4980392156862745, 0.054901960784313725)) for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  zorder=z)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9944610431097385\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.829127597085213\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.019081567754038582 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.01821704294103877 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.016879208384971055 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.522975, dev-loss=1.302034, mean-recent-eval=-0.001414\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.153918, dev-loss=0.179193, mean-recent-eval=0.008774\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.142161, dev-loss=0.164497, mean-recent-eval=0.011412\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.121445, dev-loss=0.142248, mean-recent-eval=0.012597\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.097666, dev-loss=0.113250, mean-recent-eval=0.013920\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.067296, dev-loss=0.081691, mean-recent-eval=0.015309\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.038450, dev-loss=0.039517, mean-recent-eval=0.016597\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.023330, dev-loss=0.023419, mean-recent-eval=0.017899\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.017329, dev-loss=0.012802, mean-recent-eval=0.019018\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.016991, dev-loss=0.007705, mean-recent-eval=0.019847\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.019612, dev-loss=0.019102, mean-recent-eval=0.020352\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.019375, dev-loss=0.015935, mean-recent-eval=0.020356\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.019345, dev-loss=0.013735, mean-recent-eval=0.020425\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.019357, dev-loss=0.013620, mean-recent-eval=0.020463\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.019471, dev-loss=0.012664, mean-recent-eval=0.020585\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.019703, dev-loss=0.010680, mean-recent-eval=0.020619\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.019420, dev-loss=0.014367, mean-recent-eval=0.020769\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.019545, dev-loss=0.016057, mean-recent-eval=0.020910\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.019022, dev-loss=0.009909, mean-recent-eval=0.020949\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.019247, dev-loss=0.014918, mean-recent-eval=0.021050\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.019098, dev-loss=0.014034, mean-recent-eval=0.021130\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.019092, dev-loss=0.013708, mean-recent-eval=0.021222\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.019347, dev-loss=0.012694, mean-recent-eval=0.021298\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.019716, dev-loss=0.016913, mean-recent-eval=0.021332\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.019647, dev-loss=0.013354, mean-recent-eval=0.021388\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.020002, dev-loss=0.016525, mean-recent-eval=0.021513\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.020665, dev-loss=0.015689, mean-recent-eval=0.021548\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.021054, dev-loss=0.013314, mean-recent-eval=0.021619\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.021547, dev-loss=0.017688, mean-recent-eval=0.021701\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.021923, dev-loss=0.017403, mean-recent-eval=0.021681\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.022650, dev-loss=0.016501, mean-recent-eval=0.021746\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.023058, dev-loss=0.020749, mean-recent-eval=0.021850\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.023612, dev-loss=0.018541, mean-recent-eval=0.021916\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.024063, dev-loss=0.014658, mean-recent-eval=0.021972\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.024481, dev-loss=0.017534, mean-recent-eval=0.022049\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.025344, dev-loss=0.015207, mean-recent-eval=0.022106\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.025271, dev-loss=0.016172, mean-recent-eval=0.022123\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.025202, dev-loss=0.019114, mean-recent-eval=0.022261\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.025287, dev-loss=0.020614, mean-recent-eval=0.022327\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.026810, dev-loss=0.014125, mean-recent-eval=0.022360\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.026481, dev-loss=0.016082, mean-recent-eval=0.022408\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.026684, dev-loss=0.015686, mean-recent-eval=0.022519\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.026100, dev-loss=0.016055, mean-recent-eval=0.022607\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.025742, dev-loss=0.018254, mean-recent-eval=0.022655\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.026604, dev-loss=0.015761, mean-recent-eval=0.022803\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.026121, dev-loss=0.016079, mean-recent-eval=0.022856\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.025792, dev-loss=0.019632, mean-recent-eval=0.022893\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.025729, dev-loss=0.017605, mean-recent-eval=0.023010\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.026164, dev-loss=0.016593, mean-recent-eval=0.023016\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.025722, dev-loss=0.018658, mean-recent-eval=0.023079\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=0.027620, dev-loss=0.013088, mean-recent-eval=0.023228\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.026953, dev-loss=0.015079, mean-recent-eval=0.023244\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=0.026916, dev-loss=0.013413, mean-recent-eval=0.023338\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=0.026482, dev-loss=0.014588, mean-recent-eval=0.023370\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.025622, dev-loss=0.016802, mean-recent-eval=0.023413\n",
      "iteration 5500, dev-MSE=0.000000, train-loss=0.026131, dev-loss=0.014556, mean-recent-eval=0.023594\n",
      "iteration 5600, dev-MSE=0.000000, train-loss=0.025722, dev-loss=0.016161, mean-recent-eval=0.023665\n",
      "iteration 5700, dev-MSE=0.000000, train-loss=0.026753, dev-loss=0.013649, mean-recent-eval=0.023696\n",
      "iteration 5800, dev-MSE=0.000000, train-loss=0.026523, dev-loss=0.013768, mean-recent-eval=0.023744\n",
      "iteration 5900, dev-MSE=0.000000, train-loss=0.026597, dev-loss=0.013202, mean-recent-eval=0.023830\n",
      "best iteration: 5980\n",
      "p-value: 0.9688460172730837\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8511081085847616\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0023272255132315493 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0023352163558877986 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.001860785977798312 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.394809, dev-loss=0.355955, mean-recent-eval=-0.044818\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.021696, dev-loss=0.016871, mean-recent-eval=-0.018061\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.003727, dev-loss=0.003916, mean-recent-eval=0.000672\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.004192, dev-loss=0.004007, mean-recent-eval=0.001330\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.004001, dev-loss=0.003590, mean-recent-eval=0.001337\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.004050, dev-loss=0.003287, mean-recent-eval=0.001443\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.004218, dev-loss=0.003217, mean-recent-eval=0.001478\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.004326, dev-loss=0.003406, mean-recent-eval=0.001477\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.003624, dev-loss=0.003467, mean-recent-eval=0.001555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9748746083294929\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8795372541996855\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.011353091912318525 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.012627235167457842 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.013341108616626733 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.353197, dev-loss=-0.284220, mean-recent-eval=0.001347\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.244036, dev-loss=0.285732, mean-recent-eval=-0.000390\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.207456, dev-loss=0.238471, mean-recent-eval=0.001647\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.166022, dev-loss=0.203382, mean-recent-eval=0.005209\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.145968, dev-loss=0.189198, mean-recent-eval=0.008656\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.138309, dev-loss=0.183545, mean-recent-eval=0.011434\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.131680, dev-loss=0.175965, mean-recent-eval=0.012294\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.123952, dev-loss=0.165734, mean-recent-eval=0.012158\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.117368, dev-loss=0.156921, mean-recent-eval=0.011953\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.109969, dev-loss=0.146693, mean-recent-eval=0.011727\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.102414, dev-loss=0.136360, mean-recent-eval=0.011493\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.097281, dev-loss=0.129332, mean-recent-eval=0.011268\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.089569, dev-loss=0.118649, mean-recent-eval=0.011039\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.082517, dev-loss=0.108828, mean-recent-eval=0.010811\n",
      "best iteration: 560\n",
      "p-value: 0.9883650387318034\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9813317041007668\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.08390168888227421 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.023714353611506263 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.009532372454895272 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.462115, dev-loss=-0.419299, mean-recent-eval=-0.079592\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.155540, dev-loss=0.138654, mean-recent-eval=-0.086825\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.189477, dev-loss=0.175388, mean-recent-eval=-0.047509\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.207065, dev-loss=0.199398, mean-recent-eval=-0.007574\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.190142, dev-loss=0.190481, mean-recent-eval=0.012311\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.183061, dev-loss=0.186487, mean-recent-eval=0.013004\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.174213, dev-loss=0.179087, mean-recent-eval=0.012154\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.167734, dev-loss=0.172697, mean-recent-eval=0.008980\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.158360, dev-loss=0.161815, mean-recent-eval=0.005777\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.147512, dev-loss=0.150828, mean-recent-eval=0.002527\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.138671, dev-loss=0.141415, mean-recent-eval=-0.000800\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.132509, dev-loss=0.134736, mean-recent-eval=-0.004189\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.123061, dev-loss=0.124234, mean-recent-eval=-0.007654\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.113483, dev-loss=0.113735, mean-recent-eval=-0.011207\n",
      "best iteration: 520\n",
      "p-value: 0.9974924336442615\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8825360755358904\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.006056369500959752 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.004743436060536363 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.003969497073056804 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=3.210291, dev-loss=3.103009, mean-recent-eval=-0.360404\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.357615, dev-loss=0.334391, mean-recent-eval=-0.200377\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.008842, dev-loss=0.006584, mean-recent-eval=-0.018637\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.007820, dev-loss=0.010311, mean-recent-eval=0.005450\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.006901, dev-loss=0.008093, mean-recent-eval=0.005479\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.009154, dev-loss=0.008135, mean-recent-eval=0.005581\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.008951, dev-loss=0.007539, mean-recent-eval=0.005643\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.008933, dev-loss=0.007334, mean-recent-eval=0.005708\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.008094, dev-loss=0.006262, mean-recent-eval=0.005782\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007079, dev-loss=0.007412, mean-recent-eval=0.005807\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.006956, dev-loss=0.007025, mean-recent-eval=0.005890\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.008944, dev-loss=0.008180, mean-recent-eval=0.006089\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.008455, dev-loss=0.004567, mean-recent-eval=0.006126\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.007330, dev-loss=0.004491, mean-recent-eval=0.006177\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.005562, dev-loss=0.004521, mean-recent-eval=0.006330\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.007773, dev-loss=0.005341, mean-recent-eval=0.006371\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.006559, dev-loss=0.004321, mean-recent-eval=0.006511\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.005595, dev-loss=0.004162, mean-recent-eval=0.006551\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.006166, dev-loss=0.002798, mean-recent-eval=0.006607\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.007467, dev-loss=0.009556, mean-recent-eval=0.006671\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.005369, dev-loss=0.004496, mean-recent-eval=0.006697\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.004987, dev-loss=0.003226, mean-recent-eval=0.006806\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.006341, dev-loss=0.006746, mean-recent-eval=0.006873\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.005399, dev-loss=0.005198, mean-recent-eval=0.006962\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.005248, dev-loss=0.006516, mean-recent-eval=0.007028\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.006184, dev-loss=0.006508, mean-recent-eval=0.007106\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.005820, dev-loss=0.006735, mean-recent-eval=0.007191\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.005072, dev-loss=0.004651, mean-recent-eval=0.007222\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.005528, dev-loss=0.007119, mean-recent-eval=0.007241\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.004544, dev-loss=0.002013, mean-recent-eval=0.007384\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.003419, dev-loss=-0.000155, mean-recent-eval=0.007459\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.005457, dev-loss=0.006987, mean-recent-eval=0.007495\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.003991, dev-loss=0.003603, mean-recent-eval=0.007575\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.003901, dev-loss=0.004327, mean-recent-eval=0.007650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.005509, dev-loss=0.008128, mean-recent-eval=0.007656\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.003645, dev-loss=0.001186, mean-recent-eval=0.007840\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.003717, dev-loss=0.003760, mean-recent-eval=0.007904\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.002848, dev-loss=-0.000103, mean-recent-eval=0.007956\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.004000, dev-loss=0.004320, mean-recent-eval=0.007972\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.004117, dev-loss=0.002237, mean-recent-eval=0.008089\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.003366, dev-loss=0.002402, mean-recent-eval=0.008073\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.003762, dev-loss=0.003296, mean-recent-eval=0.008241\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.003421, dev-loss=0.003389, mean-recent-eval=0.008252\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.002856, dev-loss=0.003198, mean-recent-eval=0.008343\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.002760, dev-loss=-0.000873, mean-recent-eval=0.008397\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.002555, dev-loss=-0.001273, mean-recent-eval=0.008459\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.002909, dev-loss=0.002476, mean-recent-eval=0.008474\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.001960, dev-loss=0.000209, mean-recent-eval=0.008545\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.002058, dev-loss=0.002815, mean-recent-eval=0.008648\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.003166, dev-loss=0.002206, mean-recent-eval=0.008684\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=0.002673, dev-loss=0.004026, mean-recent-eval=0.008774\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.002598, dev-loss=-0.000322, mean-recent-eval=0.008850\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=0.004526, dev-loss=0.007719, mean-recent-eval=0.008846\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=0.001385, dev-loss=-0.000402, mean-recent-eval=0.008988\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.002258, dev-loss=0.001318, mean-recent-eval=0.009043\n",
      "iteration 5500, dev-MSE=0.000000, train-loss=0.001638, dev-loss=0.000718, mean-recent-eval=0.009097\n",
      "iteration 5600, dev-MSE=0.000000, train-loss=0.002927, dev-loss=0.005400, mean-recent-eval=0.009115\n",
      "iteration 5700, dev-MSE=0.000000, train-loss=0.001287, dev-loss=-0.000471, mean-recent-eval=0.009200\n",
      "iteration 5800, dev-MSE=0.000000, train-loss=0.001929, dev-loss=0.002919, mean-recent-eval=0.009312\n",
      "iteration 5900, dev-MSE=0.000000, train-loss=0.001225, dev-loss=-0.000368, mean-recent-eval=0.009349\n",
      "best iteration: 5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.004297, dev-loss=0.003298, mean-recent-eval=0.001604\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004409, dev-loss=0.003491, mean-recent-eval=0.001631\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.004380, dev-loss=0.003528, mean-recent-eval=0.001643\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.003402, dev-loss=0.002949, mean-recent-eval=0.001698\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.003779, dev-loss=0.003449, mean-recent-eval=0.001763\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.004267, dev-loss=0.002940, mean-recent-eval=0.001762\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.004562, dev-loss=0.002337, mean-recent-eval=0.001832\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.003965, dev-loss=0.002652, mean-recent-eval=0.001868\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.003633, dev-loss=0.002921, mean-recent-eval=0.001909\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.003778, dev-loss=0.002946, mean-recent-eval=0.001940\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.003749, dev-loss=0.002274, mean-recent-eval=0.002028\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.003799, dev-loss=0.002755, mean-recent-eval=0.002005\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.003591, dev-loss=0.003307, mean-recent-eval=0.002061\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.003760, dev-loss=0.003148, mean-recent-eval=0.002077\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.003809, dev-loss=0.003144, mean-recent-eval=0.002127\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.004582, dev-loss=0.000943, mean-recent-eval=0.002209\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.003345, dev-loss=0.002047, mean-recent-eval=0.002211\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.004362, dev-loss=0.001454, mean-recent-eval=0.002264\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.003187, dev-loss=0.003581, mean-recent-eval=0.002259\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.003190, dev-loss=0.002999, mean-recent-eval=0.002330\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.003686, dev-loss=0.003192, mean-recent-eval=0.002365\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.003796, dev-loss=0.001820, mean-recent-eval=0.002373\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.003224, dev-loss=0.003191, mean-recent-eval=0.002412\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.003605, dev-loss=0.001795, mean-recent-eval=0.002476\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.003792, dev-loss=0.002587, mean-recent-eval=0.002454\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.003473, dev-loss=0.002428, mean-recent-eval=0.002498\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.003882, dev-loss=0.002375, mean-recent-eval=0.002560\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.003452, dev-loss=0.001825, mean-recent-eval=0.002586\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.003240, dev-loss=0.001816, mean-recent-eval=0.002599\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.003905, dev-loss=0.000595, mean-recent-eval=0.002645\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.003561, dev-loss=0.002579, mean-recent-eval=0.002636\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.003627, dev-loss=0.001560, mean-recent-eval=0.002749\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.003406, dev-loss=0.000934, mean-recent-eval=0.002710\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.003336, dev-loss=0.001837, mean-recent-eval=0.002755\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.002842, dev-loss=0.001484, mean-recent-eval=0.002771\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.002998, dev-loss=0.000123, mean-recent-eval=0.002808\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.002740, dev-loss=0.002009, mean-recent-eval=0.002848\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.002883, dev-loss=0.000729, mean-recent-eval=0.002859\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.003403, dev-loss=0.000090, mean-recent-eval=0.002921\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.002820, dev-loss=0.000545, mean-recent-eval=0.002936\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.002839, dev-loss=0.001620, mean-recent-eval=0.002956\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=0.003174, dev-loss=0.001487, mean-recent-eval=0.003005\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.002734, dev-loss=0.001315, mean-recent-eval=0.002985\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=0.003030, dev-loss=0.001299, mean-recent-eval=0.003035\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=0.002943, dev-loss=-0.000276, mean-recent-eval=0.003085\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.002992, dev-loss=0.001147, mean-recent-eval=0.003095\n",
      "iteration 5500, dev-MSE=0.000000, train-loss=0.002896, dev-loss=0.000698, mean-recent-eval=0.003163\n",
      "iteration 5600, dev-MSE=0.000000, train-loss=0.002292, dev-loss=0.000136, mean-recent-eval=0.003181\n",
      "iteration 5700, dev-MSE=0.000000, train-loss=0.002845, dev-loss=-0.000008, mean-recent-eval=0.003202\n",
      "iteration 5800, dev-MSE=0.000000, train-loss=0.002694, dev-loss=0.000188, mean-recent-eval=0.003235\n",
      "iteration 5900, dev-MSE=0.000000, train-loss=0.002172, dev-loss=0.001698, mean-recent-eval=0.003238\n",
      "best iteration: 5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9593187184913574\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9569135492735874\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.020363230355783545 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.020197229602722022 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.018602877866458728 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.182078, dev-loss=0.152674, mean-recent-eval=0.000746\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.171015, dev-loss=0.157464, mean-recent-eval=0.001661\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.154413, dev-loss=0.159351, mean-recent-eval=0.003477\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.138451, dev-loss=0.139820, mean-recent-eval=0.005561\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.087356, dev-loss=0.098998, mean-recent-eval=0.007879\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.054235, dev-loss=0.063181, mean-recent-eval=0.010372\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.026335, dev-loss=0.033883, mean-recent-eval=0.012969\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.011804, dev-loss=0.012708, mean-recent-eval=0.015501\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.002664, dev-loss=0.004439, mean-recent-eval=0.017773\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001342, dev-loss=0.002471, mean-recent-eval=0.019537\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004517, dev-loss=0.000454, mean-recent-eval=0.020185\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.007346, dev-loss=0.004751, mean-recent-eval=0.019590\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.010640, dev-loss=0.001139, mean-recent-eval=0.018767\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.010584, dev-loss=0.003731, mean-recent-eval=0.018422\n",
      "best iteration: 960\n",
      "p-value: 0.9949381654497977\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.929771128460989\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.08862934363907457 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.08701147277002433 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.06720635759481719 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-2.115055, dev-loss=-1.983057, mean-recent-eval=-0.063962\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.266885, dev-loss=0.257762, mean-recent-eval=-0.094868\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.189811, dev-loss=0.198428, mean-recent-eval=0.034060\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.161961, dev-loss=0.167742, mean-recent-eval=0.041120\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.132295, dev-loss=0.132269, mean-recent-eval=0.047067\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.105343, dev-loss=0.100154, mean-recent-eval=0.053573\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.076369, dev-loss=0.063429, mean-recent-eval=0.060679\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.050718, dev-loss=0.037616, mean-recent-eval=0.068203\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.034181, dev-loss=0.022425, mean-recent-eval=0.075851\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.025549, dev-loss=0.017156, mean-recent-eval=0.083152\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.022327, dev-loss=0.016221, mean-recent-eval=0.089597\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.022546, dev-loss=0.014534, mean-recent-eval=0.094807\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.024320, dev-loss=0.017699, mean-recent-eval=0.097161\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.026187, dev-loss=0.016824, mean-recent-eval=0.096927\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.027507, dev-loss=0.022410, mean-recent-eval=0.096932\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.027301, dev-loss=0.019948, mean-recent-eval=0.097024\n",
      "best iteration: 1140\n",
      "p-value: 0.9053059498056553\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9312148500660604\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0018278535814048994 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.002367372962467433 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.002225162843429216 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.178096, dev-loss=0.169903, mean-recent-eval=-0.120350\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.166224, dev-loss=0.155828, mean-recent-eval=-0.083966\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.030462, dev-loss=0.026729, mean-recent-eval=-0.029166\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.007484, dev-loss=0.006345, mean-recent-eval=0.000401\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.006257, dev-loss=0.005703, mean-recent-eval=0.002604\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.006622, dev-loss=0.005932, mean-recent-eval=0.002548\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.008630, dev-loss=0.007089, mean-recent-eval=0.002488\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.006749, dev-loss=0.006084, mean-recent-eval=0.002432\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.006784, dev-loss=0.006494, mean-recent-eval=0.002382\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007937, dev-loss=0.006786, mean-recent-eval=0.002328\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.008109, dev-loss=0.006482, mean-recent-eval=0.002274\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.008956, dev-loss=0.007486, mean-recent-eval=0.002218\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.006371, dev-loss=0.006759, mean-recent-eval=0.002169\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.007558, dev-loss=0.006818, mean-recent-eval=0.002123\n",
      "best iteration: 280\n",
      "p-value: 0.9810422338677194\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8741295868920191\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.002031233007379397 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0023288867048296006 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.002357677964789477 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.169268, dev-loss=0.134989, mean-recent-eval=-0.088657\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.007633, dev-loss=-0.006021, mean-recent-eval=-0.016400\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.000765, dev-loss=0.006470, mean-recent-eval=-0.002737\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.001116, dev-loss=-0.003329, mean-recent-eval=-0.002658\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.001468, dev-loss=-0.002718, mean-recent-eval=-0.002613\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.000319, dev-loss=0.008744, mean-recent-eval=-0.002579\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.000567, dev-loss=0.007127, mean-recent-eval=-0.002605\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.000451, dev-loss=0.004076, mean-recent-eval=-0.002595\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.001060, dev-loss=0.002122, mean-recent-eval=-0.002451\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001122, dev-loss=0.001799, mean-recent-eval=-0.002478\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000555, dev-loss=0.006331, mean-recent-eval=-0.002495p-value: 0.9847675659659717\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9295572916956075\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.047059832581677494 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.047356254734885486 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.04143532158494968 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.870630, dev-loss=-0.867658, mean-recent-eval=-0.284549\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.522532, dev-loss=0.521243, mean-recent-eval=-0.261130\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.366682, dev-loss=0.364504, mean-recent-eval=-0.156425\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.251368, dev-loss=0.247861, mean-recent-eval=-0.057686\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.186557, dev-loss=0.181323, mean-recent-eval=0.005674\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.177224, dev-loss=0.170521, mean-recent-eval=0.011628\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.164286, dev-loss=0.156905, mean-recent-eval=0.014156\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.152795, dev-loss=0.145231, mean-recent-eval=0.016558\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.134967, dev-loss=0.127841, mean-recent-eval=0.019032\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.117022, dev-loss=0.110739, mean-recent-eval=0.021613\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.095256, dev-loss=0.089990, mean-recent-eval=0.024311\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.079948, dev-loss=0.075660, mean-recent-eval=0.027061\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.061593, dev-loss=0.058167, mean-recent-eval=0.029855\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.046374, dev-loss=0.043724, mean-recent-eval=0.032755\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.035184, dev-loss=0.033286, mean-recent-eval=0.035712\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.024892, dev-loss=0.023418, mean-recent-eval=0.038678\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.018328, dev-loss=0.018004, mean-recent-eval=0.041635\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.014253, dev-loss=0.014359, mean-recent-eval=0.044349\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.012602, dev-loss=0.013080, mean-recent-eval=0.046778\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.012630, dev-loss=0.013315, mean-recent-eval=0.048703\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.013183, dev-loss=0.014418, mean-recent-eval=0.049631\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.013611, dev-loss=0.014958, mean-recent-eval=0.049531\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.013962, dev-loss=0.015534, mean-recent-eval=0.049439\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.014141, dev-loss=0.015781, mean-recent-eval=0.049484\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.014326, dev-loss=0.015833, mean-recent-eval=0.049395\n",
      "best iteration: 2000\n",
      "p-value: 0.9661436451104102\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9523945236447529\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.001202435628581384 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0022056002165405132 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.002669507659472707 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.378343, dev-loss=0.372033, mean-recent-eval=-0.212332\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.276166, dev-loss=0.265230, mean-recent-eval=-0.169174\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.179731, dev-loss=0.170349, mean-recent-eval=-0.107579\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.099112, dev-loss=0.091387, mean-recent-eval=-0.059923\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.049703, dev-loss=0.043322, mean-recent-eval=-0.025894\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.014992, dev-loss=0.010991, mean-recent-eval=-0.005867\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.009261, dev-loss=0.006685, mean-recent-eval=0.002424\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.009664, dev-loss=0.007221, mean-recent-eval=0.002695\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.008984, dev-loss=0.006797, mean-recent-eval=0.002678\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.008780, dev-loss=0.006695, mean-recent-eval=0.002644\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.008551, dev-loss=0.006639, mean-recent-eval=0.002611\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.008867, dev-loss=0.006753, mean-recent-eval=0.002561\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.009167, dev-loss=0.006943, mean-recent-eval=0.002549\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.008393, dev-loss=0.006678, mean-recent-eval=0.002503\n",
      "best iteration: 580\n",
      "p-value: 0.9651797135704265\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8767078697120558\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0009226851650099623 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0009685884375075994 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0009503062057966806 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=2.206330, dev-loss=2.141724, mean-recent-eval=-0.518954\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.612555, dev-loss=0.590135, mean-recent-eval=-0.434307\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.385848, dev-loss=0.367152, mean-recent-eval=-0.283265\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.195417, dev-loss=0.182720, mean-recent-eval=-0.139705\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.049690, dev-loss=0.046649, mean-recent-eval=-0.044440\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.004370, dev-loss=0.009447, mean-recent-eval=-0.003808\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.003393, dev-loss=0.008713, mean-recent-eval=0.000939\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.004167, dev-loss=0.009974, mean-recent-eval=0.000939\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.003144, dev-loss=0.008367, mean-recent-eval=0.000947\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.003466, dev-loss=0.008645, mean-recent-eval=0.000947\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.002912, dev-loss=0.008338, mean-recent-eval=0.000940\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.003319, dev-loss=0.008368, mean-recent-eval=0.000951\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.004100, dev-loss=0.009956, mean-recent-eval=0.000938\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.003625, dev-loss=0.008512, mean-recent-eval=0.000932\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.002422, dev-loss=0.006829, mean-recent-eval=0.000924\n",
      "best iteration: 1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000992, dev-loss=0.001704, mean-recent-eval=-0.002338\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000874, dev-loss=0.000580, mean-recent-eval=-0.002390\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000408, dev-loss=0.006981, mean-recent-eval=-0.002414\n",
      "best iteration: 80\n",
      "p-value: 0.9931832644184525\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9657190377029107\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.014234800720000936 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.013718134264150358 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.009256831884668002 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.258405, dev-loss=0.300629, mean-recent-eval=-0.093202\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.040292, dev-loss=0.052146, mean-recent-eval=-0.018146\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.035566, dev-loss=0.045966, mean-recent-eval=0.002791\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.026781, dev-loss=0.028660, mean-recent-eval=0.003763\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.026783, dev-loss=0.036982, mean-recent-eval=0.004687\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.023360, dev-loss=0.032026, mean-recent-eval=0.005702\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.019170, dev-loss=0.029903, mean-recent-eval=0.006685\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.014521, dev-loss=0.027648, mean-recent-eval=0.007697\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.010660, dev-loss=0.012333, mean-recent-eval=0.008748\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007377, dev-loss=0.015407, mean-recent-eval=0.009704\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.005385, dev-loss=0.012527, mean-recent-eval=0.010605\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.003897, dev-loss=0.007615, mean-recent-eval=0.011546\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002606, dev-loss=0.000470, mean-recent-eval=0.012347\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000888, dev-loss=0.007026, mean-recent-eval=0.013058\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000492, dev-loss=0.006428, mean-recent-eval=0.013729\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000257, dev-loss=0.005000, mean-recent-eval=0.014177\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000248, dev-loss=0.005444, mean-recent-eval=0.014723\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000091, dev-loss=0.003890, mean-recent-eval=0.015023\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000100, dev-loss=-0.001221, mean-recent-eval=0.015192\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000057, dev-loss=0.003126, mean-recent-eval=0.015339\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000537, dev-loss=-0.005721, mean-recent-eval=0.015480\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=-0.000009, dev-loss=-0.000584, mean-recent-eval=0.015540\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000176, dev-loss=0.001530, mean-recent-eval=0.015503\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000413, dev-loss=-0.005943, mean-recent-eval=0.015643\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000365, dev-loss=0.004808, mean-recent-eval=0.015581\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000046, dev-loss=0.003982, mean-recent-eval=0.015598\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000041, dev-loss=-0.002635, mean-recent-eval=0.015601\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000001, dev-loss=-0.000945, mean-recent-eval=0.015644\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000059, dev-loss=-0.000858, mean-recent-eval=0.015611\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000131, dev-loss=0.003743, mean-recent-eval=0.015649\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000191, dev-loss=-0.004255, mean-recent-eval=0.015614\n",
      "best iteration: 2640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9344191196439596\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8763396548871347\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0009149615913210649 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0005282470099901622 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.00023660637882131068 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.902881, dev-loss=0.828786, mean-recent-eval=-0.214023\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.043852, dev-loss=0.032383, mean-recent-eval=-0.073744\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.004172, dev-loss=0.001366, mean-recent-eval=-0.000257\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.002313, dev-loss=0.003320, mean-recent-eval=-0.000189\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.002857, dev-loss=0.002307, mean-recent-eval=-0.000167\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.002038, dev-loss=0.007653, mean-recent-eval=-0.000016\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.003487, dev-loss=-0.000847, mean-recent-eval=0.000104\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.001388, dev-loss=0.005669, mean-recent-eval=0.000204\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.002582, dev-loss=0.001701, mean-recent-eval=0.000288\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001665, dev-loss=0.003514, mean-recent-eval=0.000341\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.001501, dev-loss=0.002832, mean-recent-eval=0.000357\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002094, dev-loss=0.002564, mean-recent-eval=0.000448\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001344, dev-loss=0.003894, mean-recent-eval=0.000442\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.002192, dev-loss=0.001593, mean-recent-eval=0.000518\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.001288, dev-loss=0.003335, mean-recent-eval=0.000513\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000472, dev-loss=0.006711, mean-recent-eval=0.000493\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.001044, dev-loss=0.004964, mean-recent-eval=0.000548\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.001162, dev-loss=0.003809, mean-recent-eval=0.000687\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.001270, dev-loss=0.003236, mean-recent-eval=0.000681\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000898, dev-loss=0.002363, mean-recent-eval=0.000699\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000913, dev-loss=0.003053, mean-recent-eval=0.000653\n",
      "best iteration: 1660\n",
      "p-value: 0.9800295580731987\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9834222772091953\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0245037476657035 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.023547959824486205 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.016621195537342072 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.200995, dev-loss=-0.215578, mean-recent-eval=-0.034612\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.033013, dev-loss=0.055254, mean-recent-eval=-0.012052\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.030377, dev-loss=0.049421, mean-recent-eval=0.012627\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.025183, dev-loss=0.039899, mean-recent-eval=0.013878\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.020671, dev-loss=0.029459, mean-recent-eval=0.015192\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.022163, dev-loss=0.039709, mean-recent-eval=0.016411\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.017003, dev-loss=0.022798, mean-recent-eval=0.017791\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.016125, dev-loss=0.033213, mean-recent-eval=0.019061\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.013438, dev-loss=0.024738, mean-recent-eval=0.020418\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.010387, dev-loss=0.010870, mean-recent-eval=0.021755\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.009389, dev-loss=0.017521, mean-recent-eval=0.022991\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.006704, dev-loss=0.012544, mean-recent-eval=0.024342\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.006146, dev-loss=0.012475, mean-recent-eval=0.025545\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.003923, dev-loss=0.013961, mean-recent-eval=0.026699\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.003435, dev-loss=0.001610, mean-recent-eval=0.027810\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.002127, dev-loss=0.013018, mean-recent-eval=0.028762\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.002274, dev-loss=-0.000182, mean-recent-eval=0.029713\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.001199, dev-loss=0.007051, mean-recent-eval=0.030575\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000898, dev-loss=0.004270, mean-recent-eval=0.031375\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000384, dev-loss=0.003348, mean-recent-eval=0.032006\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000630, dev-loss=-0.005712, mean-recent-eval=0.032635\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000041, dev-loss=0.007346, mean-recent-eval=0.033101\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000985, dev-loss=0.017949, mean-recent-eval=0.033489\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000077, dev-loss=-0.002174, mean-recent-eval=0.033925\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000100, dev-loss=0.000425, mean-recent-eval=0.034180\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000252, dev-loss=-0.002005, mean-recent-eval=0.034482\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000089, dev-loss=0.002833, mean-recent-eval=0.034677\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000069, dev-loss=-0.001017, mean-recent-eval=0.034819\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000034, dev-loss=-0.002008, mean-recent-eval=0.035033\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000792, dev-loss=-0.009478, mean-recent-eval=0.035188\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000007, dev-loss=-0.001744, mean-recent-eval=0.035240\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000287, dev-loss=-0.006881, mean-recent-eval=0.035289\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.000034, dev-loss=-0.001471, mean-recent-eval=0.035342\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.000021, dev-loss=0.004892, mean-recent-eval=0.035391\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.000208, dev-loss=-0.004804, mean-recent-eval=0.035475\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.000026, dev-loss=0.001185, mean-recent-eval=0.035467\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=0.000023, dev-loss=0.002043, mean-recent-eval=0.035530\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.000183, dev-loss=0.006976, mean-recent-eval=0.035558\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.000075, dev-loss=-0.003701, mean-recent-eval=0.035608\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=-0.000038, dev-loss=-0.003294, mean-recent-eval=0.035582\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.000095, dev-loss=0.003990, mean-recent-eval=0.035570\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=0.000288, dev-loss=0.009865, mean-recent-eval=0.035603\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.000023, dev-loss=-0.002954, mean-recent-eval=0.035632\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.000043, dev-loss=-0.007788, mean-recent-eval=0.035659\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.000221, dev-loss=0.005097, mean-recent-eval=0.035608\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=-0.000013, dev-loss=-0.002103, mean-recent-eval=0.035634\n",
      "iteration 4600, dev-MSE=0.000000, train-loss=0.000135, dev-loss=0.006509, mean-recent-eval=0.035606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 4700, dev-MSE=0.000000, train-loss=0.000371, dev-loss=-0.008819, mean-recent-eval=0.035627\n",
      "iteration 4800, dev-MSE=0.000000, train-loss=0.000071, dev-loss=0.004043, mean-recent-eval=0.035646\n",
      "iteration 4900, dev-MSE=0.000000, train-loss=0.000015, dev-loss=-0.004406, mean-recent-eval=0.035651\n",
      "iteration 5000, dev-MSE=0.000000, train-loss=-0.000002, dev-loss=-0.001036, mean-recent-eval=0.035695\n",
      "iteration 5100, dev-MSE=0.000000, train-loss=0.000050, dev-loss=0.004976, mean-recent-eval=0.035694\n",
      "iteration 5200, dev-MSE=0.000000, train-loss=-0.000001, dev-loss=-0.001030, mean-recent-eval=0.035667\n",
      "iteration 5300, dev-MSE=0.000000, train-loss=-0.000003, dev-loss=0.001957, mean-recent-eval=0.035673\n",
      "iteration 5400, dev-MSE=0.000000, train-loss=0.000041, dev-loss=-0.001802, mean-recent-eval=0.035622\n",
      "best iteration: 5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9392268000825471\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8439039758525884\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.0014996922593132163 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0010203999578972482 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0008749241709093852 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-1.414829, dev-loss=-1.676892, mean-recent-eval=-0.204927\n",
      "iteration 100, dev-MSE=0.000000, train-loss=-0.606414, dev-loss=-0.705825, mean-recent-eval=-0.252921\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.334492, dev-loss=0.356881, mean-recent-eval=-0.302815\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.309717, dev-loss=0.328515, mean-recent-eval=-0.230486\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.197594, dev-loss=0.208499, mean-recent-eval=-0.148622\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.113568, dev-loss=0.120276, mean-recent-eval=-0.087888\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.048121, dev-loss=0.049985, mean-recent-eval=-0.044960\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.009868, dev-loss=0.006296, mean-recent-eval=-0.016792\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.001634, dev-loss=-0.005266, mean-recent-eval=-0.003216\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001651, dev-loss=-0.005169, mean-recent-eval=-0.000937\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.001438, dev-loss=-0.004750, mean-recent-eval=-0.000988\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.001196, dev-loss=-0.003453, mean-recent-eval=-0.001029\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.001532, dev-loss=-0.005032, mean-recent-eval=-0.001029\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001240, dev-loss=-0.003625, mean-recent-eval=-0.001110\n",
      "best iteration: 860\n",
      "p-value: 0.8850024642766844\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8414933675739019\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.026085325443548885 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.025270605659930872 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.01678117572209622 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.550707, dev-loss=-0.477610, mean-recent-eval=0.004293\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.083600, dev-loss=0.069314, mean-recent-eval=0.006501\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.044616, dev-loss=0.039606, mean-recent-eval=0.012912\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.036581, dev-loss=0.039076, mean-recent-eval=0.014254\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.033732, dev-loss=0.030423, mean-recent-eval=0.015496\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.027520, dev-loss=0.029213, mean-recent-eval=0.016781\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.022113, dev-loss=0.025820, mean-recent-eval=0.018065\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.017697, dev-loss=0.022019, mean-recent-eval=0.019405\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.010393, dev-loss=0.007124, mean-recent-eval=0.020755\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007453, dev-loss=0.008778, mean-recent-eval=0.022083\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.006394, dev-loss=0.012010, mean-recent-eval=0.023363\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.003149, dev-loss=0.002952, mean-recent-eval=0.024569\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.003653, dev-loss=0.010957, mean-recent-eval=0.025676\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000890, dev-loss=-0.000459, mean-recent-eval=0.026654\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000753, dev-loss=-0.003120, mean-recent-eval=0.027479\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.001113, dev-loss=0.008180, mean-recent-eval=0.028157\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000560, dev-loss=0.005162, mean-recent-eval=0.028716\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000143, dev-loss=0.001399, mean-recent-eval=0.029135\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000123, dev-loss=-0.003290, mean-recent-eval=0.029454\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000300, dev-loss=0.006020, mean-recent-eval=0.029667\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000021, dev-loss=0.000240, mean-recent-eval=0.029832\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=-0.000037, dev-loss=-0.001066, mean-recent-eval=0.029967\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=-0.000009, dev-loss=-0.002470, mean-recent-eval=0.030043\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000036, dev-loss=0.002412, mean-recent-eval=0.030115\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000095, dev-loss=0.003081, mean-recent-eval=0.030167\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=-0.000011, dev-loss=-0.000492, mean-recent-eval=0.030214\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000461, dev-loss=0.007622, mean-recent-eval=0.030238\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000005, dev-loss=0.001568, mean-recent-eval=0.030261\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000116, dev-loss=-0.003765, mean-recent-eval=0.030269\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000019, dev-loss=-0.002103, mean-recent-eval=0.030282\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000190, dev-loss=-0.003665, mean-recent-eval=0.030318\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000005, dev-loss=0.001868, mean-recent-eval=0.030338\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.000000, dev-loss=-0.000395, mean-recent-eval=0.030333\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.000377, dev-loss=-0.006951, mean-recent-eval=0.030356\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.000065, dev-loss=-0.004390, mean-recent-eval=0.030366\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=-0.000010, dev-loss=0.001528, mean-recent-eval=0.030343\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=-0.000005, dev-loss=0.002217, mean-recent-eval=0.030337\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=0.000184, dev-loss=0.002665, mean-recent-eval=0.030368\n",
      "iteration 3800, dev-MSE=0.000000, train-loss=0.000203, dev-loss=-0.006017, mean-recent-eval=0.030374\n",
      "iteration 3900, dev-MSE=0.000000, train-loss=0.000837, dev-loss=-0.007983, mean-recent-eval=0.030351\n",
      "iteration 4000, dev-MSE=0.000000, train-loss=0.000071, dev-loss=0.002766, mean-recent-eval=0.030360\n",
      "iteration 4100, dev-MSE=0.000000, train-loss=-0.000006, dev-loss=0.000724, mean-recent-eval=0.030357\n",
      "iteration 4200, dev-MSE=0.000000, train-loss=0.000145, dev-loss=0.003749, mean-recent-eval=0.030400\n",
      "iteration 4300, dev-MSE=0.000000, train-loss=0.000006, dev-loss=-0.001809, mean-recent-eval=0.030377\n",
      "iteration 4400, dev-MSE=0.000000, train-loss=0.000993, dev-loss=0.010846, mean-recent-eval=0.030374\n",
      "iteration 4500, dev-MSE=0.000000, train-loss=0.000099, dev-loss=0.002854, mean-recent-eval=0.030360\n",
      "best iteration: 4160\n",
      "p-value: 0.862837576177189\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9088989567048423\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.004994663809384793 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.002597906001576164 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.002145253141157131 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.315875, dev-loss=-0.344507, mean-recent-eval=-0.026360"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9938568895159945\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9596121248915656\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.018368100836687763 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.018215694890044983 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.01223937883790488 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.916286, dev-loss=0.802977, mean-recent-eval=-0.088721\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.041731, dev-loss=0.045592, mean-recent-eval=-0.025341\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.034154, dev-loss=0.044134, mean-recent-eval=0.005669\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.028037, dev-loss=0.041650, mean-recent-eval=0.006777\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.024731, dev-loss=0.037728, mean-recent-eval=0.007892\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.022092, dev-loss=0.025021, mean-recent-eval=0.009044\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.017482, dev-loss=0.023044, mean-recent-eval=0.010231\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.011996, dev-loss=0.023961, mean-recent-eval=0.011418\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.009258, dev-loss=0.014334, mean-recent-eval=0.012635\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.005743, dev-loss=0.017184, mean-recent-eval=0.013830\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.004266, dev-loss=0.015693, mean-recent-eval=0.014949\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002087, dev-loss=0.003153, mean-recent-eval=0.016009\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000888, dev-loss=0.004118, mean-recent-eval=0.016943\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000743, dev-loss=-0.000008, mean-recent-eval=0.017728\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000212, dev-loss=0.003443, mean-recent-eval=0.018315\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000041, dev-loss=-0.004272, mean-recent-eval=0.018733\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000343, dev-loss=-0.006677, mean-recent-eval=0.019004\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000079, dev-loss=-0.001192, mean-recent-eval=0.019193\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000018, dev-loss=-0.005659, mean-recent-eval=0.019354\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000005, dev-loss=0.000934, mean-recent-eval=0.019420\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000078, dev-loss=-0.004907, mean-recent-eval=0.019463\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000036, dev-loss=-0.001212, mean-recent-eval=0.019534\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000013, dev-loss=0.002093, mean-recent-eval=0.019549\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000259, dev-loss=-0.005519, mean-recent-eval=0.019571\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000211, dev-loss=0.004650, mean-recent-eval=0.019564\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000243, dev-loss=0.006901, mean-recent-eval=0.019549\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000061, dev-loss=-0.003470, mean-recent-eval=0.019554\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000231, dev-loss=0.005898, mean-recent-eval=0.019535\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.001066, dev-loss=-0.011793, mean-recent-eval=0.019514\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000066, dev-loss=0.002919, mean-recent-eval=0.019514\n",
      "best iteration: 2560\n",
      "p-value: 0.8796674436123653\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9247455072177125\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.005354949344615466 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0046224821993981965 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -3.0275632563886434e-06 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.383258, dev-loss=0.462455, mean-recent-eval=-0.196972\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.029870, dev-loss=0.056088, mean-recent-eval=-0.070505\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.027381, dev-loss=0.057581, mean-recent-eval=-0.007343\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.014788, dev-loss=0.024795, mean-recent-eval=-0.004098\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.012530, dev-loss=0.023273, mean-recent-eval=-0.001785\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.012069, dev-loss=0.031869, mean-recent-eval=0.000959\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.005783, dev-loss=0.003612, mean-recent-eval=0.002889\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.002849, dev-loss=0.004842, mean-recent-eval=0.004500\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.003059, dev-loss=0.011042, mean-recent-eval=0.006313\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000619, dev-loss=-0.001317, mean-recent-eval=0.007360\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000385, dev-loss=0.004893, mean-recent-eval=0.008694\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000211, dev-loss=0.007618, mean-recent-eval=0.009525\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000776, dev-loss=-0.014133, mean-recent-eval=0.010002\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000303, dev-loss=-0.008190, mean-recent-eval=0.009930\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000154, dev-loss=0.006363, mean-recent-eval=0.009964\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000296, dev-loss=-0.006287, mean-recent-eval=0.010151\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=-0.000032, dev-loss=-0.007602, mean-recent-eval=0.010351\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000008, dev-loss=0.001385, mean-recent-eval=0.009827\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000594, dev-loss=0.013783, mean-recent-eval=0.009931\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000010, dev-loss=-0.001499, mean-recent-eval=0.010021\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000017, dev-loss=0.003825, mean-recent-eval=0.009784\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000028, dev-loss=0.003209, mean-recent-eval=0.009985\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000135, dev-loss=0.004233, mean-recent-eval=0.009978\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000325, dev-loss=0.012199, mean-recent-eval=0.010107\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000171, dev-loss=-0.006067, mean-recent-eval=0.010065\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000421, dev-loss=0.012327, mean-recent-eval=0.009964\n",
      "best iteration: 2120\n",
      "p-value: 0.9538814240698916\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9428006159919426\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.06243787710874664 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.00375811049560644 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.017653345413126825 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.402997, dev-loss=0.281076, mean-recent-eval=-0.055069\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.124615, dev-loss=0.091443, mean-recent-eval=-0.029493\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.171287, dev-loss=0.153420, mean-recent-eval=0.000750\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.154631, dev-loss=0.147636, mean-recent-eval=0.005109\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.151349, dev-loss=0.148369, mean-recent-eval=0.005839\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.148791, dev-loss=0.145523, mean-recent-eval=0.006627"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9525171711503417\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8804906509727313\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.02718680805039092 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02689226146333839 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0184883164839197 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.579177, dev-loss=0.665519, mean-recent-eval=-0.012136\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.071374, dev-loss=0.089033, mean-recent-eval=-0.004005\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.052911, dev-loss=0.051558, mean-recent-eval=0.005518\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.048011, dev-loss=0.051996, mean-recent-eval=0.007681\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.043994, dev-loss=0.046878, mean-recent-eval=0.009370\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.032682, dev-loss=0.026810, mean-recent-eval=0.011200\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.031515, dev-loss=0.020435, mean-recent-eval=0.012962\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.025600, dev-loss=0.023704, mean-recent-eval=0.014828\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.019667, dev-loss=0.022092, mean-recent-eval=0.016595\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.014798, dev-loss=0.013549, mean-recent-eval=0.018504\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.009482, dev-loss=0.007086, mean-recent-eval=0.020254\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.008488, dev-loss=0.001201, mean-recent-eval=0.021967\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.004784, dev-loss=0.003618, mean-recent-eval=0.023494\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.003474, dev-loss=0.001040, mean-recent-eval=0.024880\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.001496, dev-loss=0.007596, mean-recent-eval=0.026042\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.001820, dev-loss=-0.004545, mean-recent-eval=0.027062\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000588, dev-loss=0.007416, mean-recent-eval=0.027845\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000546, dev-loss=-0.004284, mean-recent-eval=0.028495\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000731, dev-loss=-0.005893, mean-recent-eval=0.028939\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000089, dev-loss=0.001877, mean-recent-eval=0.029298\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000100, dev-loss=0.003599, mean-recent-eval=0.029614\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000009, dev-loss=0.002811, mean-recent-eval=0.029796\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=-0.000008, dev-loss=0.004275, mean-recent-eval=0.029955\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000044, dev-loss=0.000046, mean-recent-eval=0.030061\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000094, dev-loss=0.002731, mean-recent-eval=0.030157\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000006, dev-loss=0.000982, mean-recent-eval=0.030168\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=-0.000031, dev-loss=0.002598, mean-recent-eval=0.030235\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000243, dev-loss=-0.004938, mean-recent-eval=0.030240\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000260, dev-loss=0.006297, mean-recent-eval=0.030346\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000075, dev-loss=-0.001803, mean-recent-eval=0.030302\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000001, dev-loss=-0.000120, mean-recent-eval=0.030341\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000128, dev-loss=0.002173, mean-recent-eval=0.030393\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.000006, dev-loss=-0.003145, mean-recent-eval=0.030363\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.000149, dev-loss=-0.005191, mean-recent-eval=0.030325\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.000090, dev-loss=-0.004060, mean-recent-eval=0.030360\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.000005, dev-loss=-0.000265, mean-recent-eval=0.030333\n",
      "best iteration: 3100\n",
      "p-value: 0.9418851356334981\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.905585810834282\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.022415695466896708 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02239930736328337 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.020104843237291805 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.507676, dev-loss=0.518158, mean-recent-eval=-0.003372\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.176338, dev-loss=0.149732, mean-recent-eval=-0.000350\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.145000, dev-loss=0.121058, mean-recent-eval=0.002521\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.124231, dev-loss=0.102368, mean-recent-eval=0.005104\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.095199, dev-loss=0.077321, mean-recent-eval=0.007832\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.064961, dev-loss=0.051882, mean-recent-eval=0.010663\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.036624, dev-loss=0.027810, mean-recent-eval=0.013631\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.018273, dev-loss=0.012551, mean-recent-eval=0.016400\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.005452, dev-loss=0.002872, mean-recent-eval=0.018890\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.001551, dev-loss=-0.000064, mean-recent-eval=0.020878\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000198, dev-loss=-0.000492, mean-recent-eval=0.022370\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=-0.000002, dev-loss=0.000200, mean-recent-eval=0.022879\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000213, dev-loss=0.000616, mean-recent-eval=0.022846\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000034, dev-loss=0.000713, mean-recent-eval=0.023010\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000103, dev-loss=-0.000490, mean-recent-eval=0.023034\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000126, dev-loss=-0.000536, mean-recent-eval=0.023017\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000000, dev-loss=-0.000002, mean-recent-eval=0.022921\n",
      "best iteration: 1280\n",
      "\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.161642, dev-loss=0.156676, mean-recent-eval=-0.030090\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.091341, dev-loss=0.092029, mean-recent-eval=-0.022229\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.058460, dev-loss=0.057351, mean-recent-eval=-0.012021\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.046816, dev-loss=0.044465, mean-recent-eval=-0.004708\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.044489, dev-loss=0.042002, mean-recent-eval=-0.002544\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.043660, dev-loss=0.041113, mean-recent-eval=-0.002587\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.040706, dev-loss=0.037901, mean-recent-eval=-0.002619\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.041025, dev-loss=0.038306, mean-recent-eval=-0.002694\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.040286, dev-loss=0.037795, mean-recent-eval=-0.002791\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.038933, dev-loss=0.036500, mean-recent-eval=-0.002788\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.035949, dev-loss=0.033349, mean-recent-eval=-0.002817\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.036627, dev-loss=0.034040, mean-recent-eval=-0.002890\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.034237, dev-loss=0.031641, mean-recent-eval=-0.003022\n",
      "best iteration: 460\n",
      "p-value: 0.9954003123454358\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8287446817069024\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.11377541372796723 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.1139697976395836 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.08890222919154502 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-1.863396, dev-loss=-1.580696, mean-recent-eval=-0.015728\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.115383, dev-loss=0.114502, mean-recent-eval=-0.053828\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.237113, dev-loss=0.189507, mean-recent-eval=-0.011808\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.201427, dev-loss=0.153994, mean-recent-eval=0.037238\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.202174, dev-loss=0.153640, mean-recent-eval=0.041488\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.199488, dev-loss=0.149941, mean-recent-eval=0.044484\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.188084, dev-loss=0.140300, mean-recent-eval=0.047541\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.185079, dev-loss=0.136498, mean-recent-eval=0.050690\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.178920, dev-loss=0.130466, mean-recent-eval=0.053880\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.164752, dev-loss=0.118692, mean-recent-eval=0.057159\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.153832, dev-loss=0.108964, mean-recent-eval=0.060657\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.146909, dev-loss=0.101828, mean-recent-eval=0.064381\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.129446, dev-loss=0.087917, mean-recent-eval=0.068362\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.117798, dev-loss=0.077314, mean-recent-eval=0.072591\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.100871, dev-loss=0.063895, mean-recent-eval=0.077177\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.092519, dev-loss=0.055565, mean-recent-eval=0.081995\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.075392, dev-loss=0.042295, mean-recent-eval=0.087089\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.059292, dev-loss=0.030323, mean-recent-eval=0.092414\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.044947, dev-loss=0.019968, mean-recent-eval=0.097931\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.032823, dev-loss=0.011751, mean-recent-eval=0.103586\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.021328, dev-loss=0.005151, mean-recent-eval=0.109332\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.014878, dev-loss=0.000648, mean-recent-eval=0.114983\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.009483, dev-loss=-0.002214, mean-recent-eval=0.120475\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.004266, dev-loss=-0.002643, mean-recent-eval=0.125622\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.002110, dev-loss=-0.003122, mean-recent-eval=0.130275\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000984, dev-loss=-0.002838, mean-recent-eval=0.134393\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000430, dev-loss=-0.002571, mean-recent-eval=0.137920\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000061, dev-loss=-0.000920, mean-recent-eval=0.140647\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000012, dev-loss=0.001767, mean-recent-eval=0.141912\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000026, dev-loss=0.000106, mean-recent-eval=0.141846\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000054, dev-loss=0.001885, mean-recent-eval=0.142013\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000055, dev-loss=0.001260, mean-recent-eval=0.142171\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.000099, dev-loss=0.002446, mean-recent-eval=0.141985\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.000016, dev-loss=-0.000261, mean-recent-eval=0.142073\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.000085, dev-loss=0.001476, mean-recent-eval=0.142130\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.000071, dev-loss=-0.001358, mean-recent-eval=0.142123\n",
      "best iteration: 3120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9612865208177804\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.955347253427315\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.03385290286843744 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.033194830407223395 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.02240688177046745 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.363118, dev-loss=1.434632, mean-recent-eval=-0.110840\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.047877, dev-loss=0.035779, mean-recent-eval=-0.026387\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.045150, dev-loss=0.029664, mean-recent-eval=0.011157\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.036460, dev-loss=0.021466, mean-recent-eval=0.013171\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.033469, dev-loss=0.019669, mean-recent-eval=0.015217\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.029649, dev-loss=0.017079, mean-recent-eval=0.017319\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.023835, dev-loss=0.011719, mean-recent-eval=0.019464\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.017696, dev-loss=0.006716, mean-recent-eval=0.021651\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.010226, dev-loss=0.004137, mean-recent-eval=0.023833\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.007338, dev-loss=0.001035, mean-recent-eval=0.025935\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.005559, dev-loss=-0.000522, mean-recent-eval=0.027935\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.002665, dev-loss=-0.001387, mean-recent-eval=0.029686\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.002343, dev-loss=-0.001177, mean-recent-eval=0.031168\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.001681, dev-loss=-0.001617, mean-recent-eval=0.032555\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000735, dev-loss=-0.001406, mean-recent-eval=0.033700\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000110, dev-loss=-0.000747, mean-recent-eval=0.034622\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000422, dev-loss=-0.000618, mean-recent-eval=0.035370\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000123, dev-loss=-0.000173, mean-recent-eval=0.035706\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000545, dev-loss=-0.000134, mean-recent-eval=0.036071\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000044, dev-loss=0.000096, mean-recent-eval=0.036394\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000000, dev-loss=-0.000074, mean-recent-eval=0.036542\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000090, dev-loss=-0.000170, mean-recent-eval=0.036648\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000282, dev-loss=0.000425, mean-recent-eval=0.036800\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000147, dev-loss=-0.000170, mean-recent-eval=0.036806\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000044, dev-loss=-0.000336, mean-recent-eval=0.036852\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=-0.000002, dev-loss=0.000246, mean-recent-eval=0.036885\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000117, dev-loss=0.000057, mean-recent-eval=0.036916\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000157, dev-loss=0.000395, mean-recent-eval=0.036831\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000064, dev-loss=0.000223, mean-recent-eval=0.036857\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=-0.000003, dev-loss=0.000123, mean-recent-eval=0.037005\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=-0.000008, dev-loss=0.000135, mean-recent-eval=0.036918\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000324, dev-loss=0.000304, mean-recent-eval=0.036988\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.000078, dev-loss=-0.000112, mean-recent-eval=0.036965\n",
      "best iteration: 2820\n",
      "p-value: 0.8492715419192032\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8628149925065876\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.026800365229005117 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.012663988566625043 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.008288274551010437 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.310996, dev-loss=1.550667, mean-recent-eval=-0.147957\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.378873, dev-loss=0.468928, mean-recent-eval=-0.120384\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.142820, dev-loss=0.176338, mean-recent-eval=-0.078760\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.122880, dev-loss=0.148694, mean-recent-eval=-0.040798\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.119762, dev-loss=0.140396, mean-recent-eval=-0.012969\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.116387, dev-loss=0.135206, mean-recent-eval=-0.008998\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.112555, dev-loss=0.131323, mean-recent-eval=-0.008959\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.110711, dev-loss=0.128139, mean-recent-eval=-0.009083\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.107776, dev-loss=0.126367, mean-recent-eval=-0.009147\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.102559, dev-loss=0.118855, mean-recent-eval=-0.009411\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.097892, dev-loss=0.113795, mean-recent-eval=-0.009635\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.096184, dev-loss=0.111385, mean-recent-eval=-0.009646\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.091674, dev-loss=0.106052, mean-recent-eval=-0.009909\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.085683, dev-loss=0.099057, mean-recent-eval=-0.010127\n",
      "best iteration: 480\n",
      "p-value: 0.9845262519405107\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8506007568107552\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.07694429901600248 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.07675356319650455 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.05643397084424688 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.139467, dev-loss=0.140725, mean-recent-eval=0.019535\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.138912, dev-loss=0.129989, mean-recent-eval=0.022004\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.140064, dev-loss=0.129399, mean-recent-eval=0.027127\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.115403, dev-loss=0.105401, mean-recent-eval=0.031870\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.098853, dev-loss=0.088657, mean-recent-eval=0.037229\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.069799, dev-loss=0.060941, mean-recent-eval=0.043132\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.042390, dev-loss=0.035125, mean-recent-eval=0.049777\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.026276, dev-loss=0.020075, mean-recent-eval=0.056735\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.010863, dev-loss=0.006666, mean-recent-eval=0.063840\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.004527, dev-loss=0.001521, mean-recent-eval=0.070407\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000877, dev-loss=-0.000077, mean-recent-eval=0.076121\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000222, dev-loss=0.001305, mean-recent-eval=0.080266\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000469, dev-loss=-0.001155, mean-recent-eval=0.081850\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000063, dev-loss=-0.000104, mean-recent-eval=0.082132\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000268, dev-loss=0.001314, mean-recent-eval=0.082037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.144561, dev-loss=0.140735, mean-recent-eval=0.007521\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.140143, dev-loss=0.137365, mean-recent-eval=0.008436\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.132129, dev-loss=0.130141, mean-recent-eval=0.009371\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.130657, dev-loss=0.128891, mean-recent-eval=0.010314\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.124184, dev-loss=0.120040, mean-recent-eval=0.011276\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.117039, dev-loss=0.117169, mean-recent-eval=0.012263\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.113622, dev-loss=0.111625, mean-recent-eval=0.013276\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.106545, dev-loss=0.107204, mean-recent-eval=0.014319\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.100025, dev-loss=0.099648, mean-recent-eval=0.015395\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.095089, dev-loss=0.095835, mean-recent-eval=0.015618\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.087921, dev-loss=0.088158, mean-recent-eval=0.012470\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.080281, dev-loss=0.081762, mean-recent-eval=0.008899\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.072932, dev-loss=0.074917, mean-recent-eval=0.005228\n",
      "best iteration: 1440\n",
      "p-value: 0.9980184819183174\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.6672148373406992\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.05740291948850307 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.05755701724967642 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.04488844040832969 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.899553, dev-loss=0.942072, mean-recent-eval=-0.039880\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.164793, dev-loss=0.163951, mean-recent-eval=-0.005127\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.155585, dev-loss=0.152317, mean-recent-eval=0.011663\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.155159, dev-loss=0.153226, mean-recent-eval=0.013528\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.141380, dev-loss=0.138881, mean-recent-eval=0.015484\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.136320, dev-loss=0.134182, mean-recent-eval=0.017514\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.124496, dev-loss=0.122754, mean-recent-eval=0.019631\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.120625, dev-loss=0.119989, mean-recent-eval=0.021848\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.104635, dev-loss=0.102714, mean-recent-eval=0.024171\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.093311, dev-loss=0.090854, mean-recent-eval=0.026624\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.082006, dev-loss=0.081140, mean-recent-eval=0.029208\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.066635, dev-loss=0.066109, mean-recent-eval=0.031911\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.056586, dev-loss=0.055258, mean-recent-eval=0.034729\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.043492, dev-loss=0.042752, mean-recent-eval=0.037646\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.031860, dev-loss=0.030971, mean-recent-eval=0.040625\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.023373, dev-loss=0.022375, mean-recent-eval=0.043634\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.014451, dev-loss=0.013943, mean-recent-eval=0.046632\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.008919, dev-loss=0.008588, mean-recent-eval=0.049549\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.004615, dev-loss=0.004444, mean-recent-eval=0.052311\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.002718, dev-loss=0.002517, mean-recent-eval=0.054851\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000820, dev-loss=0.000666, mean-recent-eval=0.057114\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000283, dev-loss=0.000193, mean-recent-eval=0.059033\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000011, dev-loss=0.000053, mean-recent-eval=0.060472\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000008, dev-loss=-0.000019, mean-recent-eval=0.061055\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=-0.000005, dev-loss=-0.000015, mean-recent-eval=0.061224\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=-0.000029, dev-loss=-0.000014, mean-recent-eval=0.061301\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=-0.000002, dev-loss=0.000002, mean-recent-eval=0.061246\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000010, dev-loss=0.000045, mean-recent-eval=0.061198\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=-0.000004, dev-loss=-0.000001, mean-recent-eval=0.061211\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000044, dev-loss=0.000009, mean-recent-eval=0.061310\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.000000, dev-loss=-0.000008, mean-recent-eval=0.061252\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.000126, dev-loss=0.000095, mean-recent-eval=0.061253\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=-0.000004, dev-loss=0.000041, mean-recent-eval=0.061263\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.000027, dev-loss=0.000045, mean-recent-eval=0.061238\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.000061, dev-loss=0.000063, mean-recent-eval=0.061316\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.000019, dev-loss=-0.000029, mean-recent-eval=0.061247\n",
      "iteration 3600, dev-MSE=0.000000, train-loss=-0.000003, dev-loss=-0.000011, mean-recent-eval=0.061296\n",
      "iteration 3700, dev-MSE=0.000000, train-loss=-0.000002, dev-loss=0.000019, mean-recent-eval=0.061194\n",
      "best iteration: 3320\n",
      "p-value: 0.9596961728683386\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9589303100030686\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.14165958610300847 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.1417436100360009 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.10957553716749091 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=1.296267, dev-loss=1.336682, mean-recent-eval=-0.041583\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.249783, dev-loss=0.252365, mean-recent-eval=0.004003\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.246954, dev-loss=0.244971, mean-recent-eval=0.044327\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.239133, dev-loss=0.237070, mean-recent-eval=0.049417\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.229840, dev-loss=0.227951, mean-recent-eval=0.054471\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.213724, dev-loss=0.212096, mean-recent-eval=0.059835\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.195099, dev-loss=0.193314, mean-recent-eval=0.065549\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.179342, dev-loss=0.178238, mean-recent-eval=0.071630\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.150768, dev-loss=0.149581, mean-recent-eval=0.078150\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.128908, dev-loss=0.127571, mean-recent-eval=0.085085\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.103152, dev-loss=0.102002, mean-recent-eval=0.092411\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.078195, dev-loss=0.078166, mean-recent-eval=0.100112\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.056233, dev-loss=0.056739, mean-recent-eval=0.108145\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.035440, dev-loss=0.036476, mean-recent-eval=0.116428\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.019628, dev-loss=0.020451, mean-recent-eval=0.124824\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.007825, dev-loss=0.009437, mean-recent-eval=0.133035\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.002770, dev-loss=0.003089, mean-recent-eval=0.140641\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000245, dev-loss=0.001043, mean-recent-eval=0.147030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9856915213749604\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9442955582900491\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.015138919867112952 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.026146167662252598 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.026741148466115597 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-1.156920, dev-loss=-1.423919, mean-recent-eval=-0.162247\n",
      "iteration 100, dev-MSE=0.000000, train-loss=-0.168074, dev-loss=-0.233347, mean-recent-eval=-0.191579\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.307966, dev-loss=0.358036, mean-recent-eval=-0.206005\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.227065, dev-loss=0.268621, mean-recent-eval=-0.155410\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.176572, dev-loss=0.208348, mean-recent-eval=-0.107942\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.138696, dev-loss=0.156989, mean-recent-eval=-0.065555\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.123639, dev-loss=0.129002, mean-recent-eval=-0.025709\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.120790, dev-loss=0.123843, mean-recent-eval=-0.000355\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.120658, dev-loss=0.124550, mean-recent-eval=0.004286\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.118294, dev-loss=0.122070, mean-recent-eval=0.007036\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.109109, dev-loss=0.112195, mean-recent-eval=0.009346\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.110180, dev-loss=0.115601, mean-recent-eval=0.010349\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.102417, dev-loss=0.105894, mean-recent-eval=0.011243\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.099044, dev-loss=0.104120, mean-recent-eval=0.012162\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.096007, dev-loss=0.102498, mean-recent-eval=0.013102\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.090116, dev-loss=0.097589, mean-recent-eval=0.014088\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.082912, dev-loss=0.087631, mean-recent-eval=0.015101\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.078964, dev-loss=0.084946, mean-recent-eval=0.016155\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.073643, dev-loss=0.079770, mean-recent-eval=0.017239\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.068772, dev-loss=0.075864, mean-recent-eval=0.018353\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.062173, dev-loss=0.067246, mean-recent-eval=0.019513\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.056497, dev-loss=0.063802, mean-recent-eval=0.020699\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.050843, dev-loss=0.059866, mean-recent-eval=0.021920\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.044437, dev-loss=0.053050, mean-recent-eval=0.023158\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.038225, dev-loss=0.045966, mean-recent-eval=0.024440\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.033523, dev-loss=0.039668, mean-recent-eval=0.025748\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.027542, dev-loss=0.033789, mean-recent-eval=0.027068\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.021772, dev-loss=0.030492, mean-recent-eval=0.028415\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.018065, dev-loss=0.027351, mean-recent-eval=0.029766\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.013989, dev-loss=0.016839, mean-recent-eval=0.031131\n",
      "iteration 3000, dev-MSE=0.000000, train-loss=0.010039, dev-loss=0.013912, mean-recent-eval=0.032498\n",
      "iteration 3100, dev-MSE=0.000000, train-loss=0.006922, dev-loss=0.012995, mean-recent-eval=0.033836\n",
      "iteration 3200, dev-MSE=0.000000, train-loss=0.004698, dev-loss=0.004191, mean-recent-eval=0.034905\n",
      "iteration 3300, dev-MSE=0.000000, train-loss=0.002963, dev-loss=0.004924, mean-recent-eval=0.034051\n",
      "iteration 3400, dev-MSE=0.000000, train-loss=0.001552, dev-loss=0.005957, mean-recent-eval=0.032939\n",
      "iteration 3500, dev-MSE=0.000000, train-loss=0.000534, dev-loss=0.004471, mean-recent-eval=0.031324\n",
      "best iteration: 3180\n",
      "p-value: 0.9720324634041964\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8752312587257695\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.14349089696272072 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.14415744306975342 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.11318977513953031 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-1.627979, dev-loss=-1.764750, mean-recent-eval=0.001565\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.416792, dev-loss=0.440238, mean-recent-eval=-0.012697\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.301172, dev-loss=0.314538, mean-recent-eval=0.025774\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.273364, dev-loss=0.275712, mean-recent-eval=0.041599\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.270041, dev-loss=0.265901, mean-recent-eval=0.044087\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.252644, dev-loss=0.246889, mean-recent-eval=0.048581\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.249864, dev-loss=0.245159, mean-recent-eval=0.053521\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.235202, dev-loss=0.229108, mean-recent-eval=0.058503\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.216468, dev-loss=0.208062, mean-recent-eval=0.063399\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.201890, dev-loss=0.193969, mean-recent-eval=0.068603\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.182428, dev-loss=0.173759, mean-recent-eval=0.074155\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.158297, dev-loss=0.148322, mean-recent-eval=0.080053\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.133592, dev-loss=0.122893, mean-recent-eval=0.086340\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.109713, dev-loss=0.099311, mean-recent-eval=0.093006\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.085476, dev-loss=0.075966, mean-recent-eval=0.100058\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.060670, dev-loss=0.051006, mean-recent-eval=0.107434\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.041240, dev-loss=0.033174, mean-recent-eval=0.115118\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.024032, dev-loss=0.018121, mean-recent-eval=0.123005\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.013080, dev-loss=0.006845, mean-recent-eval=0.130824\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.005525, dev-loss=0.001509, mean-recent-eval=0.138324\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.001599, dev-loss=-0.000355, mean-recent-eval=0.144918\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000160, dev-loss=0.000487, mean-recent-eval=0.150392\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=-0.000001, dev-loss=0.000588, mean-recent-eval=0.153895\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000111, dev-loss=-0.000595, mean-recent-eval=0.154704\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000031, dev-loss=-0.000731, mean-recent-eval=0.154690\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000030, dev-loss=-0.000706, mean-recent-eval=0.154660\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000002, dev-loss=0.000370, mean-recent-eval=0.154761\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000005, dev-loss=-0.000380, mean-recent-eval=0.154683\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000016, dev-loss=-0.000456, mean-recent-eval=0.154509\n",
      "iteration 2900, dev-MSE=0.000000, train-loss=0.000118, dev-loss=-0.001169, mean-recent-eval=0.154615\n",
      "best iteration: 2560\n",
      "p-value: 0.974760362716299\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8855400899075754\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2p-value: 0.9580790272205534\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.7670593096919258\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.05887087606422651 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.05995934723708503 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.0533001684997935 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.087635, dev-loss=-0.125030, mean-recent-eval=0.016444\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.114114, dev-loss=0.154789, mean-recent-eval=0.017900\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.109933, dev-loss=0.148938, mean-recent-eval=0.020149\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.104267, dev-loss=0.142058, mean-recent-eval=0.022270\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.099644, dev-loss=0.135901, mean-recent-eval=0.024441\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.092275, dev-loss=0.126184, mean-recent-eval=0.026730\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.085541, dev-loss=0.117335, mean-recent-eval=0.029072\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.080081, dev-loss=0.110294, mean-recent-eval=0.031571\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.070548, dev-loss=0.097359, mean-recent-eval=0.034191\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.064860, dev-loss=0.089471, mean-recent-eval=0.036926\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.056303, dev-loss=0.078893, mean-recent-eval=0.039794\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.048346, dev-loss=0.068098, mean-recent-eval=0.042850\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.041332, dev-loss=0.059186, mean-recent-eval=0.046059\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.033019, dev-loss=0.047114, mean-recent-eval=0.049396\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.027221, dev-loss=0.040724, mean-recent-eval=0.052809\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.021268, dev-loss=0.029238, mean-recent-eval=0.056316\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.014988, dev-loss=0.023056, mean-recent-eval=0.059865\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.010001, dev-loss=0.016525, mean-recent-eval=0.063387\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.006026, dev-loss=0.010687, mean-recent-eval=0.066824\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.004007, dev-loss=0.006260, mean-recent-eval=0.070131\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.002101, dev-loss=0.005073, mean-recent-eval=0.073203\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.001251, dev-loss=0.001753, mean-recent-eval=0.076059\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000430, dev-loss=-0.000414, mean-recent-eval=0.078538\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=0.000232, dev-loss=-0.000722, mean-recent-eval=0.077064\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000104, dev-loss=-0.001741, mean-recent-eval=0.076039\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=-0.000004, dev-loss=0.000027, mean-recent-eval=0.075147\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=-0.000027, dev-loss=-0.003006, mean-recent-eval=0.073980\n",
      "best iteration: 2200\n",
      "p-value: 0.9942421195099065\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9280278943908878\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.09026241032796291 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.026089074781643778 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0010147080515433143 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.791430, dev-loss=-0.875241, mean-recent-eval=-0.167225\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.275326, dev-loss=0.293517, mean-recent-eval=-0.194436\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.282120, dev-loss=0.304549, mean-recent-eval=-0.159745\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.251077, dev-loss=0.268739, mean-recent-eval=-0.103209\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.232173, dev-loss=0.244197, mean-recent-eval=-0.059398\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.230045, dev-loss=0.237765, mean-recent-eval=-0.030177\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.231800, dev-loss=0.239646, mean-recent-eval=-0.016636\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.219236, dev-loss=0.225498, mean-recent-eval=-0.013308\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.219749, dev-loss=0.226709, mean-recent-eval=-0.011061\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.206606, dev-loss=0.211967, mean-recent-eval=-0.009719\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.203873, dev-loss=0.209981, mean-recent-eval=-0.010077\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.190784, dev-loss=0.194598, mean-recent-eval=-0.012255\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.188872, dev-loss=0.194502, mean-recent-eval=-0.014670\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.177216, dev-loss=0.181515, mean-recent-eval=-0.017089\n",
      "best iteration: 900\n",
      "p-value: 0.9804472201778962\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.860066421122343\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.1270749609065382 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.1271683990846432 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.10320650218466304 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.666624, dev-loss=0.750049, mean-recent-eval=0.013601\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.300015, dev-loss=0.304545, mean-recent-eval=0.014858\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.292472, dev-loss=0.294508, mean-recent-eval=0.018832\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.296885, dev-loss=0.298467, mean-recent-eval=0.023493\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.279481, dev-loss=0.277760, mean-recent-eval=0.028320\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.271608, dev-loss=0.267475, mean-recent-eval=0.033260\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.258458, dev-loss=0.252258, mean-recent-eval=0.038435\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.234307, dev-loss=0.223846, mean-recent-eval=0.043853\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.224806, dev-loss=0.212797, mean-recent-eval=0.049560\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.195783, dev-loss=0.180342, mean-recent-eval=0.055575\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.174336, dev-loss=0.156960, mean-recent-eval=0.061912\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.145736, dev-loss=0.127473, mean-recent-eval=0.068552\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.118407, dev-loss=0.098607, mean-recent-eval=0.075489\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.091337, dev-loss=0.071075, mean-recent-eval=0.082682\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.067278, dev-loss=0.047329, mean-recent-eval=0.090082\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.043979, dev-loss=0.026043, mean-recent-eval=0.097572\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.025066, dev-loss=0.010644, mean-recent-eval=0.105125\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.014927, dev-loss=0.002663, mean-recent-eval=0.112528\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.006046, dev-loss=-0.002367, mean-recent-eval=0.119520\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.001420, dev-loss=-0.002688, mean-recent-eval=0.125788\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000192, dev-loss=-0.001580, mean-recent-eval=0.130948\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000335, dev-loss=-0.000705, mean-recent-eval=0.081945\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=-0.000012, dev-loss=0.000428, mean-recent-eval=0.082036\n",
      "best iteration: 1280\n",
      "p-value: 0.997422835345572\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9394600460425795\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.0751969500133925 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.07472778664447197 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.05648502447130699 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-0.199509, dev-loss=-0.174305, mean-recent-eval=-0.064004\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.284517, dev-loss=0.286890, mean-recent-eval=-0.022994\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.245108, dev-loss=0.253600, mean-recent-eval=0.011984\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.219629, dev-loss=0.227632, mean-recent-eval=0.023712\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.184632, dev-loss=0.192141, mean-recent-eval=0.035423\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.138371, dev-loss=0.144413, mean-recent-eval=0.048015\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.079253, dev-loss=0.086130, mean-recent-eval=0.057443\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.037252, dev-loss=0.048467, mean-recent-eval=0.067282\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.012124, dev-loss=0.017253, mean-recent-eval=0.077263\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.002137, dev-loss=0.008051, mean-recent-eval=0.083343\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000107, dev-loss=-0.002554, mean-recent-eval=0.085743\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=-0.000003, dev-loss=-0.002546, mean-recent-eval=0.087102\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000139, dev-loss=0.003344, mean-recent-eval=0.087687\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000205, dev-loss=0.005792, mean-recent-eval=0.087730\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.000124, dev-loss=0.002511, mean-recent-eval=0.087784\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000257, dev-loss=-0.006626, mean-recent-eval=0.086741\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.000023, dev-loss=0.001568, mean-recent-eval=0.087092\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.000106, dev-loss=0.002604, mean-recent-eval=0.087642\n",
      "best iteration: 1340\n",
      "p-value: 0.9914612072549652\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9896407577246916\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.048796167851764834 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.04882647681242942 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.04301397098738252 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.285065, dev-loss=0.225806, mean-recent-eval=0.006379\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.025927, dev-loss=0.023119, mean-recent-eval=0.006019\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.261026, dev-loss=0.263674, mean-recent-eval=0.006413\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.239076, dev-loss=0.238970, mean-recent-eval=0.007286\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.223322, dev-loss=0.223864, mean-recent-eval=0.008822\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.206122, dev-loss=0.207386, mean-recent-eval=0.010547\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.208148, dev-loss=0.209740, mean-recent-eval=0.012370\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.199015, dev-loss=0.201382, mean-recent-eval=0.014340\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.181709, dev-loss=0.184829, mean-recent-eval=0.016468\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.163765, dev-loss=0.167478, mean-recent-eval=0.018751\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.147105, dev-loss=0.151757, mean-recent-eval=0.021212\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.126369, dev-loss=0.132232, mean-recent-eval=0.023825\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.102462, dev-loss=0.108645, mean-recent-eval=0.026586\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.080766, dev-loss=0.086034, mean-recent-eval=0.029481\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.056008, dev-loss=0.060120, mean-recent-eval=0.032487\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.039268, dev-loss=0.043593, mean-recent-eval=0.035585\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.022874, dev-loss=0.025865, mean-recent-eval=0.038709\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.011092, dev-loss=0.015151, mean-recent-eval=0.041763\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.004604, dev-loss=0.006484, mean-recent-eval=0.044640\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000971, dev-loss=0.003169, mean-recent-eval=0.047150\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000192, dev-loss=0.001073, mean-recent-eval=0.049124\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=-0.000033, dev-loss=0.002569, mean-recent-eval=0.050345\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=-0.000021, dev-loss=-0.000352, mean-recent-eval=0.050596\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=-0.000009, dev-loss=-0.001701, mean-recent-eval=0.050575\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000012, dev-loss=0.000779, mean-recent-eval=0.050599\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=-0.000002, dev-loss=-0.000225, mean-recent-eval=0.050598\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=0.000006, dev-loss=0.000187, mean-recent-eval=0.050622\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000070, dev-loss=-0.001694, mean-recent-eval=0.050509\n",
      "best iteration: 2380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "DeepGMM/optimizers/oadam.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272178570/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.000018, dev-loss=0.000504, mean-recent-eval=0.151420\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000058, dev-loss=0.001081, mean-recent-eval=0.152581\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.000014, dev-loss=0.001026, mean-recent-eval=0.152678\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.000184, dev-loss=-0.001179, mean-recent-eval=0.152623\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000000, dev-loss=0.000501, mean-recent-eval=0.152517\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=-0.000007, dev-loss=0.000180, mean-recent-eval=0.152564\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000009, dev-loss=0.000629, mean-recent-eval=0.152620\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000008, dev-loss=0.000409, mean-recent-eval=0.152622\n",
      "iteration 2600, dev-MSE=0.000000, train-loss=-0.000000, dev-loss=-0.001113, mean-recent-eval=0.152518\n",
      "iteration 2700, dev-MSE=0.000000, train-loss=0.000005, dev-loss=0.000135, mean-recent-eval=0.152389\n",
      "iteration 2800, dev-MSE=0.000000, train-loss=0.000021, dev-loss=-0.000327, mean-recent-eval=0.152642\n",
      "best iteration: 2400\n",
      "p-value: 0.9987741148592835\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.969784049536724\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.12030959818808427 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.024024970542004323 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.0049578678456927774 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.235131, dev-loss=0.380426, mean-recent-eval=-0.022977\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.048926, dev-loss=0.078277, mean-recent-eval=-0.003362\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.241273, dev-loss=0.280749, mean-recent-eval=0.010625\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.271780, dev-loss=0.323522, mean-recent-eval=-0.017601\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.263434, dev-loss=0.315761, mean-recent-eval=-0.032607\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.258561, dev-loss=0.310489, mean-recent-eval=-0.031216\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.251471, dev-loss=0.302368, mean-recent-eval=-0.026613\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.247371, dev-loss=0.297423, mean-recent-eval=-0.021592\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.241063, dev-loss=0.290056, mean-recent-eval=-0.016240\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.240736, dev-loss=0.290571, mean-recent-eval=-0.012867\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.234487, dev-loss=0.283409, mean-recent-eval=-0.014483\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.225594, dev-loss=0.272666, mean-recent-eval=-0.016627\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.215063, dev-loss=0.260741, mean-recent-eval=-0.018778\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.210492, dev-loss=0.255298, mean-recent-eval=-0.021016\n",
      "best iteration: 140\n",
      "p-value: 0.9840238091844474\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.8562621314451713\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.17659545532191406 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: -0.027596078500001647 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.013157223437480534 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.895332, dev-loss=0.833387, mean-recent-eval=-0.102468\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.152502, dev-loss=0.138005, mean-recent-eval=-0.081715\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.329518, dev-loss=0.354579, mean-recent-eval=-0.050889\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.356309, dev-loss=0.386124, mean-recent-eval=-0.042308\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.353549, dev-loss=0.382880, mean-recent-eval=-0.041487\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.342814, dev-loss=0.372121, mean-recent-eval=-0.036852\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.348711, dev-loss=0.377533, mean-recent-eval=-0.032245\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.338618, dev-loss=0.367788, mean-recent-eval=-0.027026\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.331355, dev-loss=0.360182, mean-recent-eval=-0.022222\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.333672, dev-loss=0.362031, mean-recent-eval=-0.017175\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.316862, dev-loss=0.345144, mean-recent-eval=-0.011866\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.311828, dev-loss=0.339693, mean-recent-eval=-0.005994\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.300965, dev-loss=0.328856, mean-recent-eval=-0.006867\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.296933, dev-loss=0.324176, mean-recent-eval=-0.011620\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.281360, dev-loss=0.308360, mean-recent-eval=-0.016558\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.269461, dev-loss=0.295937, mean-recent-eval=-0.021727\n",
      "best iteration: 1100\n",
      "p-value: 0.980328075368809\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9138229595391832\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.07880994486277063 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.07890267746581071 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.07368944905301462 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.878848, dev-loss=0.800112, mean-recent-eval=-0.155963\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.399110, dev-loss=0.367240, mean-recent-eval=-0.119428\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.358624, dev-loss=0.336852, mean-recent-eval=-0.063108\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.344029, dev-loss=0.327908, mean-recent-eval=-0.025495\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.343898, dev-loss=0.327501, mean-recent-eval=-0.013249\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.336018, dev-loss=0.319986, mean-recent-eval=-0.005320\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.314755, dev-loss=0.301705, mean-recent-eval=0.003782\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.306829, dev-loss=0.293117, mean-recent-eval=0.012183\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.290542, dev-loss=0.277417, mean-recent-eval=0.020868\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.272881, dev-loss=0.260382, mean-recent-eval=0.030361\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.245164, dev-loss=0.233767, mean-recent-eval=0.040217\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.210006, dev-loss=0.202422, mean-recent-eval=0.050581\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.185870, dev-loss=0.176613, mean-recent-eval=0.060722\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.144175, dev-loss=0.139626, mean-recent-eval=0.071264\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.107354, dev-loss=0.100702, mean-recent-eval=0.081460\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.072987, dev-loss=0.069432, mean-recent-eval=0.090693\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.040581, dev-loss=0.038817, mean-recent-eval=0.096329\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.018405, dev-loss=0.018900, mean-recent-eval=0.100152\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.004629, dev-loss=0.004578, mean-recent-eval=0.101138\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.000323, dev-loss=-0.001518, mean-recent-eval=0.100279\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=-0.000005, dev-loss=0.000252, mean-recent-eval=0.134532\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.000003, dev-loss=0.000096, mean-recent-eval=0.135810\n",
      "iteration 2300, dev-MSE=0.000000, train-loss=-0.000004, dev-loss=-0.001508, mean-recent-eval=0.135756\n",
      "iteration 2400, dev-MSE=0.000000, train-loss=0.000028, dev-loss=-0.001023, mean-recent-eval=0.135778\n",
      "iteration 2500, dev-MSE=0.000000, train-loss=0.000002, dev-loss=0.000557, mean-recent-eval=0.135786\n",
      "best iteration: 2160\n",
      "p-value: 0.9947391831189689\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.9595087859253859\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: -0.03251791231504404 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.011329224704708988 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.011501213324513423 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=-1.340265, dev-loss=-1.201193, mean-recent-eval=-0.151026\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.262453, dev-loss=0.289575, mean-recent-eval=-0.165280\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.464572, dev-loss=0.436969, mean-recent-eval=-0.158676\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.406905, dev-loss=0.386604, mean-recent-eval=-0.129492\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.353974, dev-loss=0.343527, mean-recent-eval=-0.101651\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.339360, dev-loss=0.337735, mean-recent-eval=-0.074159\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.328426, dev-loss=0.335062, mean-recent-eval=-0.047550\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.337972, dev-loss=0.347131, mean-recent-eval=-0.030635\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.326217, dev-loss=0.337767, mean-recent-eval=-0.024642\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.332650, dev-loss=0.341812, mean-recent-eval=-0.020970\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.316927, dev-loss=0.326808, mean-recent-eval=-0.017642\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.305814, dev-loss=0.317731, mean-recent-eval=-0.014724\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.305159, dev-loss=0.315390, mean-recent-eval=-0.011430\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.303413, dev-loss=0.312269, mean-recent-eval=-0.008376\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=0.294225, dev-loss=0.302609, mean-recent-eval=-0.005283\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.275097, dev-loss=0.284755, mean-recent-eval=-0.001877\n",
      "iteration 1600, dev-MSE=0.000000, train-loss=0.270267, dev-loss=0.278653, mean-recent-eval=0.001481\n",
      "iteration 1700, dev-MSE=0.000000, train-loss=0.256103, dev-loss=0.263603, mean-recent-eval=0.004847\n",
      "iteration 1800, dev-MSE=0.000000, train-loss=0.245474, dev-loss=0.253512, mean-recent-eval=0.008421\n",
      "iteration 1900, dev-MSE=0.000000, train-loss=0.231490, dev-loss=0.237432, mean-recent-eval=0.008580\n",
      "iteration 2000, dev-MSE=0.000000, train-loss=0.218299, dev-loss=0.223458, mean-recent-eval=0.005378\n",
      "iteration 2100, dev-MSE=0.000000, train-loss=0.206334, dev-loss=0.210690, mean-recent-eval=0.002237\n",
      "iteration 2200, dev-MSE=0.000000, train-loss=0.185134, dev-loss=0.188944, mean-recent-eval=-0.000945\n",
      "best iteration: 1820\n",
      "p-value: 0.9222172713647272\n",
      "iteration 1, accept the test!\n",
      "p-value: 0.895575352875807\n",
      "iteration 1, accept the test!\n",
      "starting learning args eval 0\n",
      "starting learning args eval 1\n",
      "starting learning args eval 2\n",
      "learning eval: 0.12661232657961075 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0025:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.12625917405737672 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0002:'betas'=(0.5, 0.9) OAdam:::'lr'=0.001:'betas'=(0.5, 0.9)\n",
      "learning eval: 0.10831261881669738 OptimalObjective::lambda_1=0.250000 OAdam:::'lr'=0.0001:'betas'=(0.5, 0.9) OAdam:::'lr'=0.0005:'betas'=(0.5, 0.9)\n",
      "size of f_z collection: 300\n",
      "iteration 0, dev-MSE=0.000000, train-loss=0.357330, dev-loss=0.331472, mean-recent-eval=0.025631\n",
      "iteration 100, dev-MSE=0.000000, train-loss=0.380032, dev-loss=0.360413, mean-recent-eval=0.032666\n",
      "iteration 200, dev-MSE=0.000000, train-loss=0.314254, dev-loss=0.295805, mean-recent-eval=0.043132\n",
      "iteration 300, dev-MSE=0.000000, train-loss=0.262695, dev-loss=0.249810, mean-recent-eval=0.054504\n",
      "iteration 400, dev-MSE=0.000000, train-loss=0.189531, dev-loss=0.180098, mean-recent-eval=0.067984\n",
      "iteration 500, dev-MSE=0.000000, train-loss=0.108375, dev-loss=0.105856, mean-recent-eval=0.083780\n",
      "iteration 600, dev-MSE=0.000000, train-loss=0.033802, dev-loss=0.034681, mean-recent-eval=0.101460\n",
      "iteration 700, dev-MSE=0.000000, train-loss=0.003938, dev-loss=0.004190, mean-recent-eval=0.118573\n",
      "iteration 800, dev-MSE=0.000000, train-loss=0.000100, dev-loss=0.003874, mean-recent-eval=0.130002\n",
      "iteration 900, dev-MSE=0.000000, train-loss=0.000064, dev-loss=-0.002299, mean-recent-eval=0.132240\n",
      "iteration 1000, dev-MSE=0.000000, train-loss=0.000015, dev-loss=0.001114, mean-recent-eval=0.131924\n",
      "iteration 1100, dev-MSE=0.000000, train-loss=0.000032, dev-loss=-0.001366, mean-recent-eval=0.132150\n",
      "iteration 1200, dev-MSE=0.000000, train-loss=0.000027, dev-loss=0.001832, mean-recent-eval=0.132236\n",
      "iteration 1300, dev-MSE=0.000000, train-loss=0.000004, dev-loss=0.000135, mean-recent-eval=0.132100\n",
      "iteration 1400, dev-MSE=0.000000, train-loss=-0.000016, dev-loss=0.000303, mean-recent-eval=0.132179\n",
      "iteration 1500, dev-MSE=0.000000, train-loss=0.000013, dev-loss=-0.001583, mean-recent-eval=0.132060\n",
      "best iteration: 1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py:1784: UserWarning: You passed a edgecolor/edgecolors ((1.0, 0.4980392156862745, 0.054901960784313725)) for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  zorder=z)\n"
     ]
    }
   ],
   "source": [
    "# for instrument in ['Gaussian', 'Binary']:\n",
    "for instrument in ['Gaussian', 'Binary']:\n",
    "    res_df = None\n",
    "    ret_df_vis = None\n",
    "    # get a fix x_vis\n",
    "    iv_type = 'mix_{}'.format(instrument)\n",
    "    _, _, _, X_vis = gen_data(f, n, iv_type)\n",
    "\n",
    "    alphas = np.linspace(0, 1, 5)\n",
    "\n",
    "    for j in range(len(alphas)):\n",
    "        alpha = alphas[j]\n",
    "\n",
    "        def rep_function(i):\n",
    "            X, Y, Z, _ = gen_data(f, n, iv_type, alpha=alpha)\n",
    "            # dev set for DeepGMM\n",
    "            X_dev, Y_dev, Z_dev, _ = gen_data(f, n, iv_type, alpha=alpha)\n",
    "            # oracle set\n",
    "            X_o, Y_o, Z_o, _ = gen_data(f, n, iv_type, alpha=alpha, oracle=True)\n",
    "            X_test, _, _, _ = gen_data(f, X_vis.shape[0], iv_type, alpha=alpha)\n",
    "\n",
    "            # Pure predictive\n",
    "            mse_net = NonlinearModel(input_dim=1, \n",
    "                                      lr=config_mse['lr'],\n",
    "                                      lmd=-99)\n",
    "\n",
    "            mse_net = train_mse(mse_net, config_mse, X, Y, Z)\n",
    "            y_hat_mse = mse_net(to_torch(X_test)).detach().numpy()\n",
    "\n",
    "            oracle_net = train_mse(mse_net, config_mse, X_o, Y_o, Z_o)\n",
    "            y_hat_oracle = mse_net(to_torch(X_test)).detach().numpy()\n",
    "\n",
    "            # HSIC IV\n",
    "            s_z = med_sigma(Z)\n",
    "            kernel_e = RBFKernel(sigma=1)\n",
    "\n",
    "            if instrument == 'Binary':\n",
    "                kernel_z = CategoryKernel()\n",
    "            else:\n",
    "                kernel_z = RBFKernel(sigma=s_z)\n",
    "            \n",
    "            # non regularized HSIC IV\n",
    "            hsic_net = NonlinearModel(input_dim=1, \n",
    "                                      lr=config_hsic['lr'],\n",
    "                                      kernel_e=kernel_e,\n",
    "                                      kernel_z=kernel_z,\n",
    "                                      lmd=0)\n",
    "\n",
    "            hsic_net.load_state_dict(mse_net)\n",
    "            hsic_net = train_HSIC_IV(hsic_net, config_hsic, X, Y, Z, verbose=False)\n",
    "\n",
    "            intercept_adjust = Y.mean() - hsic_net(to_torch(X)).mean()\n",
    "            y_hat_hsic = intercept_adjust + hsic_net(to_torch(X_test))\n",
    "            y_hat_hsic = y_hat_hsic.detach().numpy().copy()\n",
    "            \n",
    "            # regularized HSIC IV\n",
    "            hsic_net = NonlinearModel(input_dim=1, \n",
    "                                      lr=config_hsic['lr'],\n",
    "                                      kernel_e=kernel_e,\n",
    "                                      kernel_z=kernel_z,\n",
    "                                      lmd=5e-5)\n",
    "\n",
    "            hsic_net.load_state_dict(mse_net)\n",
    "            hsic_net = train_HSIC_IV(hsic_net, config_hsic, X, Y, Z, verbose=False)\n",
    "\n",
    "            intercept_adjust = Y.mean() - hsic_net(to_torch(X)).mean()\n",
    "            y_hat_hsic_pen = intercept_adjust + hsic_net(to_torch(X_test))\n",
    "            y_hat_hsic_pen = y_hat_hsic_pen.detach().numpy().copy()\n",
    "\n",
    "            # prepare data for DeepGMM\n",
    "            dat = [X, Z, Y, X_dev, Z_dev, Y_dev]\n",
    "            # to torch\n",
    "            for k in range(len(dat)):\n",
    "                dat[k] = to_torch(dat[k]).double()\n",
    "\n",
    "            deepGMM = ToyModelSelectionMethod()\n",
    "            deepGMM.fit(*dat, g_dev=None, verbose=True)\n",
    "            y_hat_deepGMM = deepGMM.predict(to_torch(X_test).double()).flatten().detach().numpy()\n",
    "\n",
    "            inner_df = pd.DataFrame()\n",
    "            inner_df_vis = pd.DataFrame()\n",
    "\n",
    "            inner_df['f_x'] = f(X_test)\n",
    "            inner_df['Pred'] = y_hat_mse\n",
    "            inner_df['HSIC-IV'] = y_hat_hsic\n",
    "            inner_df['HSIC-AR'] = y_hat_hsic_pen\n",
    "            inner_df['D-GMM'] = y_hat_deepGMM\n",
    "            inner_df['Oracle'] = y_hat_oracle\n",
    "            inner_df['alpha'] = alpha\n",
    "            inner_df['run_id'] = i\n",
    "\n",
    "            return inner_df\n",
    "        \n",
    "        ret_df = Parallel(n_jobs=4)(\n",
    "            delayed(rep_function)(i=i) for i in range(n_rep))\n",
    "\n",
    "        # ret_df = [rep_function(i) for i in range(n_rep)]\n",
    "\n",
    "        ret_df = functools.reduce(lambda df1, df2: df1.append(df2, ignore_index=True), ret_df)\n",
    "\n",
    "        if res_df is None:\n",
    "            res_df = ret_df\n",
    "        else:\n",
    "            res_df = res_df.append(ret_df, ignore_index=True)\n",
    "\n",
    "    melt_res_df = res_df.melt(id_vars=['f_x', 'alpha', 'run_id'], var_name='Method',\n",
    "                              value_name='y_pred')\n",
    "    melt_res_df['MISE'] = (melt_res_df['f_x'] - melt_res_df['y_pred']) ** 2\n",
    "    final_df = melt_res_df.groupby(['Method', 'alpha', 'run_id'])['MISE'].mean().reset_index()\n",
    "    final_df['alpha'] = np.round(final_df.alpha, 2)\n",
    "    final_df.to_csv(\"results/compare_df_NN_ins_{}.csv\".format(instrument),\n",
    "                    index=False)\n",
    "    sns.set(font_scale=1.4, style='white', palette=sns.set_palette(\"tab10\"))\n",
    "\n",
    "    g = sns.catplot(data=final_df, kind=\"point\", log=True,\n",
    "                    x='alpha', y='MISE', hue='Method',\n",
    "                    markers=[\"o\", \"x\", \"d\", \"s\", \"v\"], linestyles=[':', '--', '-', '-.', ':'],\n",
    "                    capsize=.07, aspect=1.5, height=3.2, ci=95)\n",
    "    g.fig.get_axes()[0].set_yscale('log')\n",
    "    g._legend.remove()\n",
    "\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), \n",
    "               ncol=5, fancybox=True, shadow=False, prop={'size': 10})\n",
    "    plt.savefig('results/compare_NN_ins_{}.pdf'.format(instrument),\n",
    "                bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c73cdb0d-87f1-4d14-b721-89c90cf693dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py:1784: UserWarning: You passed a edgecolor/edgecolors ((1.0, 0.4980392156862745, 0.054901960784313725)) for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  zorder=z)\n",
      "/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py:1784: UserWarning: You passed a edgecolor/edgecolors ((1.0, 0.4980392156862745, 0.054901960784313725)) for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  zorder=z)\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.DataFrame()\n",
    "for instrument in ['Binary', 'Gaussian']:\n",
    "# for instrument in ['Gaussian']:\n",
    "    final_df = pd.read_csv(\"results/compare_df_NN_ins_{}.csv\".format(instrument))\n",
    "    final_df = final_df.replace({\"HSIC-AR\": \"HSIC-X-pen\", \n",
    "                                 \"HSIC-IV\": \"HSIC-X\",\n",
    "                                 \"Pred\":\"OLS\",\n",
    "                                 \"D-GMM\": \"DeepGMM\"})\n",
    "    final_df[r'$Z$'] = instrument\n",
    "    all_df = all_df.append(final_df, ignore_index=True)\n",
    "    \n",
    "sns.set(font_scale=1.8, style='white', palette=sns.set_palette(\"tab10\"))\n",
    "\n",
    "g = sns.catplot(data=all_df, kind=\"point\", log=True,\n",
    "                x='alpha', y='MISE', hue='Method',\n",
    "                hue_order=['DeepGMM', 'OLS', 'Oracle', 'HSIC-X', 'HSIC-X-pen'],\n",
    "                markers=[\"o\", \"x\", \"d\", \"s\", \"v\"], linestyles=[':', '-', '--', '-.', ':'],\n",
    "                capsize=.07, aspect=1.2, height=4, ci=95,\n",
    "                col=r'$Z$', sharey=False)\n",
    "\n",
    "g._legend.remove()\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(-.25, 1.43), \n",
    "  ncol=5, fancybox=True, shadow=True, prop={'size': 15.5})\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "g.set_xlabels(r'$\\alpha$')\n",
    "g.set_ylabels('MSE')\n",
    "\n",
    "plt.savefig('results/compare_NN.pdf',\n",
    "            bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
